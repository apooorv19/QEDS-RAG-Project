Please provide the OCR text, and I'll be happy to help you clean it up according to the system instructions.

Outline of the course

• Module 1: Review of Classical Linear Regression Model (CLRM), violations of assumptions, heteroskedasticity and autocorrelation
• Module 2: Multicollinearity, Ordinary Least Squares Bias, Instrumental Variables
• Module 3: Time series data analysis - introduction
• Module 4: Stationarity in Time series
• Module 5: Forecasting
Lecture 1: Basic Econometrics

Outline of Lecture

Scope of Econometrics
Econometrics is a branch of economics that aims to provide empirical evidence for economic theories and models. It uses statistical methods to analyze data and test hypotheses about economic phenomena.

Difference from Statistical Inference
While econometrics shares some similarities with statistical inference, there are key differences. Statistical inference focuses on making inferences about populations based on samples, whereas econometrics is concerned with using data to test economic theories and models.

Some Examples
For instance, econometrics might be used to analyze the impact of a monetary policy change on inflation or the effect of a tax cut on GDP. In these cases, econometrics provides a framework for testing hypotheses about the relationships between variables.

Structure of Economic Data
Economic data typically takes the form of time series or cross-sectional data. Time series data shows changes over time, while cross-sectional data compares different groups or observations at a single point in time.

Causality and Ceteris Paribus

The concept of causality is central to many fields, including philosophy, economics, and science. However, the notion of causality is often complicated by the fact that we can never fully control for all variables in a given situation. This is where the idea of ceteris paribus comes in.

Ceteris paribus is a Latin phrase that translates to "all other things being equal." It is used to describe a hypothetical scenario in which all variables except one are held constant, allowing us to isolate the effect of that one variable. In other words, it allows us to control for all other factors and see how they affect the outcome.

For example, imagine we want to know whether a certain medicine causes a particular symptom. We could give the medicine to a group of people and then compare them to a group who did not receive the medicine. However, there are many variables that could affect the outcome, such as age, health status, and lifestyle. To control for these variables, we would need to match the two groups in terms of these factors.

This is where ceteris paribus comes in. By assuming that all other things are equal between the two groups, we can isolate the effect of the medicine and see whether it causes the symptom. Of course, this is a simplification, as there may be many variables that we cannot control for. However, it allows us to make some sense of complex data and draw conclusions about cause-and-effect relationships.

In conclusion, ceteris paribus is an important concept in understanding causality. It allows us to isolate the effect of one variable while controlling for all other factors. This can be useful in a wide range of fields, from medicine to economics to philosophy.

Please provide the OCR text, and I'll be happy to help you clean it up according to the system instructions.

Violations of Assumptions

Please provide the OCR text, and I'll be happy to help you rewrite it to make it readable by fixing typos, merging broken lines, and leaving math/equations alone. I'll output only the cleaned text for you!

Scope of Econometrics

• Examining the effectiveness of a government policy, job training program, welfare scheme, or subsidies, etc.
• Examine returns on investment strategies to determine whether they comply with existing theoretical models
• Econometrics: based on statistical methods for:
	+ Estimating economic relationships,
	+ Testing economic theories, and
	+ Evaluating and implementing government and business policy

Lecture 1: Basic Econometrics

Econometrics: The big picture

Sample

Economic theory
Population
Estimands
Parameters
Estimators

Identification
Statistical inference

Make life easier by separating the statistical task (inferring estimands from data) from the modeling task (picking estimands that identify parameters)

Lecture 1: Basic Econometrics

Economic model to econometric model

• Economic theory/econometric model
• Economic relationship in mind

Suppose: y = f(x, z)
and f_x' > 0

Have some data: y, x, z

Test whether the expected relationship between y and x holds given z

Econometric model/estimable equation:

y = α + βx + δz + ε
ε is unobserved factors

Is β > 0? => Hypothesis

Lecture 1: Basic Econometrics

Some Terms

Dependent Variable/Endogenous Variable/Regressand: y
Explanatory Variable/Exogenous Variable/Regressor: x
Control Variable: z

Lecture 1: Basic Econometrics

Example: Demand Function

Law of demand: quantity demanded is inversely proportional to the price of a good, ceteris paribus.

y = f(x, z)

• What is y here?
• What is x ?
• Potential z's?
• Estimable equation?

Lecture 1: Basic Econometrics

Structure of Economic Data

• Cross-sectional data: A sample of individuals, households, firms, cities, states, countries, or a variety of other units taken at a given point in time.
• Time series data: Consists of observations on a variable or several variables over time.

Please provide the OCR text, and I'll be happy to help you clean it up!

Some data sets have both cross-sectional and time-series features, by combining the two, years.

Please provide the OCR text, and I'll be happy to help you rewrite it to make it readable by fixing typos, merging broken lines, and leaving math/equations alone. I'll output only the cleaned text for you.

A set consists of a time series for each cross-sectional member in the data set. Lecture 1: Basic Econometrics

Causality and Ceteris Paribus

• x causes y: One variable has an impact on another variable.
• Some policy has an effect on certain outcomes, referred to as a causal effect.

Association might be suggestive

But establishing causality is needed to make compelling arguments. For example, an education-wage association is not sufficient. We need to establish that all else remains constant, and then we can say that x changes y.

Lecture 1: Basic Econometrics

Class discussion: Problem 1

Suppose we want to estimate the returns to years of education in terms of income.

• What will be the estimable equation? Explain the intuition behind this equation with possible sign of parameters.

Class discussion: Problem 2

Suppose you are in charge of training and placement of students at a technical institute. You are asked to find out if the number of students offered a job by a company has any relation to the average package offered by the company in the last five years. How will you carry out this exercise using data?

• What data do you need?
• What is the nature of this data?
• What is the relationship that you need to estimate? Explain.

Lecture 1: Basic Econometrics

Class discussion: Problem 3

Suppose you are asked to conduct a study to determine whether smaller class sizes lead to improved student performance of fourth graders. If you could conduct any experiment you want, what would you do? Be specific.

More realistically, suppose you can collect observational data on several thousand fourth graders in a given state. You can obtain the size of their fourth-grade class and a standardized test score taken at the end of fourth grade. Why might you expect a negative correlation between class size and test score?

Would a negative correlation necessarily show that smaller class sizes cause better performance? Explain.

Lecture 1: Basic Econometrics

Class discussion: Problem 4

Under general assumptions, we can derive an equation describing the amount of time spent in criminal activity as a function of various factors. We might represent such a function as y = f(x1, x2, x3, x4, x5, x6, x7),

where
y = hours spent in criminal activities,
x1 = "wage" for an hour spent in criminal activity,
x2 = hourly wage in legal employment,
x3 = income other than from crime or employment,
x4 = probability of getting caught,
x5 = probability of being convicted if caught,
x6 = expected sentence if convicted, and
x7 = age.

Write the relation as an estimable equation.
What are the expected signs of each of the coefficients?

Regression Recap
Lecture 1: Basic Econometrics

Linear Regression: Model with k-explanatory variables

Model: y = α1x1 + α2x2 + … + αkxk + ε

Objective: To estimate α1, α2, …, αk

Method of ordinary least squares: The estimates of α1, α2, …, αk are chosen simultaneously by minimizing:

∑(yi - α1xi - α2x2i - … - αkxki)²

Assumptions:

Linear regression model, or linear in the parameters

1. Fixed X values or X values independent of the error term
2. The covariance between each Xi and εi is zero: cov(x1i, εi) = cov(x2i, εi) = ... = 0
3. Zero mean value of disturbance εi (Conditional independence axiom): E(εi|Xi) = 0
4. Homoscedasticity or constant variance of εi: var(εi) = σ^2
5. No autocorrelation, or serial correlation, between the disturbances: cov(εi, εj) = 0 for all i ≠ j
6. The number of observations n must be greater than the number of parameters to be estimated
7. There must be variation in the values of the X variables
8. No exact collinearity between the X variables

In Matrix notation,

Y = Xβ + ε
Dimension of Y: n × 1
Dimension of X: n × k
Dimension of β: k × 1
Dimension of ε: n × 1

Assumptions:
E(ε) = 0
E(X'ε) = 0
Rank[(X'X)] = k

OLS estimates

The OLS estimator of ε'ε is given by Y'Y - 2βX'Y + βX'Xβ, where Y is a column vector of observed outcomes, X is a matrix of predictor variables, and β is the vector of regression coefficients.

The derivative of ε'ε with respect to β is equal to -2XY + 2XXβ = 0. Solving for β yields:

β = (XX')^(-1)XY

The variance-covariance matrix of the OLS estimator β is given by σ^2((XX')^(-1)), which is a k × k matrix with the sampling variances of the β's displayed on the main diagonal and the covariances in the off-diagonal positions.

The estimated variance of ε, denoted as ^σ^2, is equal to ^ε'ε/1.

BLUE Property

OLS estimates are unbiased, linear, and uncorrelated (BLUE). 
What is BLUE? 

Let me know if you need any further assistance!

Properties of OLS

Best Linear Unbiased Estimator

Unbiased: E(β) = β

Efficient: Minimum variance

Consistent: plimβ = β: The distribution of estimated βk becomes more and more tightly distributed around βk as the sample size grows.

• What happens if n → ∞?

Goodness of Fit

R2 = ESS/TSS = (∑(Yi - Ȳ)(ŷi - Ȳ̄))² / (∑(Yi - Ȳ)² ∑(ŷi - Ȳ̄)²)

= Cov(Yi, ŷi) / var(Yi)var(ŷi)

= 1 - RSS/TSS

Inference: Hypothesis testing

FIGURE 4.4: 5% rejection rule for the alternative hypothesis <math>H_1</math>: <math>\beta_1 \neq 0</math> with 25 degrees of freedom.

Individual regression coefficients:
<math>H_0: \beta_i = 0</math>
<math>H_1: \beta_i \neq 0</math>

The alternative hypothesis is two-sided.

The test statistic is given by:

<math>t = \frac{\widehat{\beta}_j - \beta_j}{se(\widehat{\beta}_i)} \sim t_{n-k}</math>

A confidence interval, which is expected to contain the true parameter with an area of 0.025, is defined as:

<math>\widehat{\beta}_j - t\alpha_{/2} se(\widehat{\beta}_j) \le \beta_j \le \widehat{\beta}_j + t\alpha_{/2} se(\widehat{\beta}_j)</math>

The results are as follows:

0: rejection
rejection
-2.06
2.06: region
region

Inference: Hypothesis testing
2. Overall significance of the sample regression:
Hypothesis testing: H0: β1 = β2 = β3 = … = βk = 0
Analysis of variance:
F = ESS/(k-1) / RSS/(n-k) ∼ Fk-1,n-k
H0 is rejected when the variables are jointly statistically significant.

Type I Error

• A Type I error is a false positive conclusion, where we reject the null hypothesis when it is actually true.
• The probability of making a Type I error is the significance level, or alpha (α).
• Level of significance: Probability of rejecting the null hypothesis when it is true.

Probability of type I error
Lecture 1: Basic Econometrics

Type II Error

• A Type II error is a false negative conclusion, where one fails to reject the null hypothesis when it is actually false.
• The power of the test is closely related to the concept of Type II error.

Truth in the Population versus Results in the Study Sample: The Four Possibilities

| Truth in the Population | Results in the Study Sample |
| --- | --- |
| Association | Reject Null Hypothesis (Correct) |
| No Association | Fail to Reject Null Hypothesis (Type I Error) |
| Association | Fail to Reject Null Hypothesis (Type II Error) |
| No Association | Reject Null Hypothesis |

Lecture 1: Basic Econometrics

Note: Cov(X, ε) = 0 implies that certain relationships among independent or explanatory variables are ruled out, and these variables have nothing to do with the error term. E[ε|X] = 0 is more important, as it restricts the relationship between the unobserved factors in ε and the explanatory variables.

Let me know if you need any further assistance!

Interpreting the OLS Regression equation: 

log(wage) = 0.284 + 0.092*edu + 0.0041*exper + 0.022*tenure
(0.104) (0.007) (0.0017)
(0.003)
n = 526
R^2 = 0.316

What percent of variation in log(wage) is explained by the variables?
Is the coefficient of edu statistically significant?

When random sampling is not enough,

Cross-section data: assume that they have been obtained by random sampling from the underlying population. This raises a sample selection problem. We sample from units that are large relative to the population, particularly geographical units - representativeness of the data is questionable.

Heteroskedasticity

• Homoskedasticity assumption: The variance of the unobserved error, ε, conditional on the explanatory variables, is constant.
• Homoskedasticity fails whenever the variance of the unobserved factors changes across different segments of the population, where the segments are determined by the different values of the explanatory variables.
• Example: Heteroskedasticity is present if the variance of the unobserved factors affecting savings increases with income.

Multicollinearity

The existence of a "perfect," or exact, linear relationship among some or all explanatory variables of a regression model

• Perfect multicollinearity: the regression coefficients of the X variables are indeterminate and their standard errors are infinite
• If multicollinearity is less than perfect, the regression coefficients, although determinate, possess large standard errors (in relation to the coefficients themselves), which means the coefficients cannot be estimated with great precision or accuracy.

Omitted variable bias
• Describes the relationship between regression estimates in models with different sets of control variables.
• Suppose we want to estimate the effect of years of schooling on earnings.

Yi = α + ρsi + ηi

But other factors? Family background, intelligence, ability (A) and motivation?

Yi = α + ρsi + Ai + ηi

Ability - observable? Measurable?
CIA is violated

Autocorrelation

Data points in a time series are related by correlation between members of a series of observations ordered in time. The covariance between consecutive errors (εi and εj) does not equal zero for all i ≠ j, indicating serial correlation.

εt = εt-1 + ut
t = 2, 3, ..., n