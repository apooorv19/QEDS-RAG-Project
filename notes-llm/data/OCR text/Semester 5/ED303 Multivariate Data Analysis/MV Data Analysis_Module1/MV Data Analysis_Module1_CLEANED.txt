Multivariate Data Analysis: Exponential Family Distribution

Sample Space E (52, 5)

T.V.: f(x) = 2 - 1R; i.c. f^(-1)(x) = (-0)

a) ∈ S (inverse image of a Borel set & algebra)

Why Right Continuity? (False)

H.W.

Can PDF be [0, 1] only? [No]!

(weight)

Euclidean Distance:

d(OP) = √(x_1^2 + x_2^2)

Statistical Distance: accounts for variation as well as dependence

χ' = χ/S_1, χ' = χ/S_2

But 22 = F(71.)

(χ_{1/2}/S_1)^2 + (χ_{1/2}/S_2)^2

OP =

4 correlation

Re = μ cos θ + y sin θ

The equation is: => x ˆy = -μ sin θ + y cos θ

**Eigenvalue Decomposition**

(A) Trace: 12 + (Sun of digital moss) 1-1110

SVD
ORD
LU
OF
Date
Page No.
> OP = √(π^2 cos^2 θ + y^2 sin^2 θ + 2πy sin θ cos θ + π^2 sin^2 θ + y^2 cos^2 θ - 2πy sin θ cos θ)
OP = √(n^2 + y^2)

7

d(0, p) = √((χ/S11)^2 + (y/S22)^2) × 2y are dependent.

[Matrix]
XAX ≥ 0

**Ranson:**

• Eigenvalues: AX = λX (Scaling factor)
[Matrix] [n1; n2] → · [x → λX]

· Eigenvectors: (A-A) X = 0 (Anisotropic) = 3.5 (treating matrix space)

· Matrix Space:
- Distance Measure -
1 d(x, y) > 0
(a) (1, y) = d(y, n)
3 Triangle Inequality: d(x,y) & d(x,z) + d(z,y)

Teacher's Signature: ...

Date: 

Page No.: 21

What is meant by mean vector and variance-covariance matrix?

Answer:

The sample mean vector is an unbiased estimator of the population mean vector.

Formula:

M = (1/n) Σ Xi

where M is the sample mean vector, n is the number of samples, and Xi are the individual data points.

Note: The formula for calculating the sample mean vector is given by:

M = (1/n) Σ Xi

where M is the sample mean vector, n is the number of samples, and Xi are the individual data points.

Also, it's worth noting that the variance-covariance matrix is a measure of the spread or dispersion of the data, and it can be used to calculate the covariance between different variables.

Data

Page No.

×
4
1
a.
Xnxp =
3
3
5
2
Sor
4.
=
Mean-Vector: u
3
=
II 2
M. = 4 + (-1) + 3 8 -2
3
<u>u_2 = 1 + 3 + 5 = 8 / 3 = 3</u>
correlation (a): [-1 < x & i] -> Sample Geometry (a.1)
V
<math display="block">\gamma = \sum (\pi i - \bar{\pi})(y_i - \bar{y}) \Rightarrow \cos \theta = 0</math>
Sn. Sy
7= cos θ ∈ [-1, ]
ì
7
•
Teacher's Signature...

2000

Date: People Number

Multivariate Random Variable Version 1-

Discrete X = (x1, x2, ..., xn)

Continuous

Probability Mass Function (PMF) of X:

Let me know if you need any further assistance!

The probability density function (PDF) of a random vector X = (X1, ..., Xn) is given by:

P(X1 = x1, X2 = x2, ..., f(x1, x2, ..., xn) = ∂/∂x1 ⋅ ∂x1 F(x1, ..., xn)

CDF of X:
CDF of Z:

The cumulative distribution function (CDF) of X is defined as:

P(X1 ≤ x1, X2 ≤ x2, ..., Xn ≤ xn) = P(X1 ≤ x1, X2 ≤ x1, Xn ≤ xn) = ... = P(X1 ≤ x1, X2 ≤ x2, ..., Xn ≤ xn)

The marginal distribution of x is given by:

Σξ, ... Σ? (x, εi, ...)

Mean of Random Vectors:-

F(X) = ∫∞ -∞ ... ∫∞ -∞ T(x)f(x)

E(x) = ∫n f(n) dn

Note: The original input contained a large number of repeated mathematical expressions, which I have condensed into the above cleaned text.

Date: The expectation of a random vector X is denoted by E(X).

X is a Random Vector and its expectation: E(x)
enists.
F(X) = u
Find the Expected Value of Y = ax + b
<Emath>E(Y) = E[ax + b]</emath>
> E(Y) = E(a)E(x) + b
= E(y) = E(a)u + b

Y = aX + b: More General Form?

The Covariance Matrix [COV(A,B) = E(AB) = E(A).E(B)]

Var (x) = E ((x-u)^2)

2.
Cov(x_i, x_j) = E[(x_i - \overline{x_i})] * (x_i - \overline{x_j})

6_{11}^{2} 6_{12}^{2} 6_{12}^{2} 6_{12}^{2} 6_{12}^{2} 6_{12}^{2} 6_{12}^{2} 6_{12}^{2} 6_{12}^{2} 6_{12}^{2} 6_{12}^{2} 6_{12}^{2}

Page No.

Phultmariat (Agrina)

E(cX_i) = cE(X_i) = cH_i

Link

Van (CXI) = C2 Van(XI) = C2611
Cov (ax., bx2) = E (ax. - u,) (bx2 - bu2)
= ab E[(x_1-u_1)(x_2-u_2)]
= ab Cov((x_1, x_2)) = ab Cov(x_1, x_2)

· E(ax, + bx2) = aE(x,) + bE(x2) = au, + bu,
· Var (ax1 + bx2) = E[(ax1+bx2) - (au, +bu,)]^2
= E[a(x,-M) + b(x2-M2)]^2
= a^2 Var(X_1) + b^2 Var(X_2) + 2ab Cov(X_1, X_2)
= a^2 σ_1 + b^2 σ_2 + 2ab ρσ_1σ_2

c' = [a,b],
(a × x_1 + b × x_2) / (a × x_1 + b × x_2) = [(x_1) (x_2)] / [(x_1) (x_2)] = c'* x_1 / a
E(ax_1 + bx_2) = au_1 + bu_2 = [ab] u_1 / u_2 = c'u

Σ: Con(X)

612

Date: _______________________
Page No.: _______________________

Since, <math>C' \geq C = \begin{bmatrix} a & b \end{bmatrix} \begin{bmatrix} G_{11} & G_{12} & G_{22} \end{bmatrix} \begin{bmatrix} a \\ b \end{bmatrix}</math>
= <math>a^2 G_{11} + 2ab G_{12} + b^2 G_{22}</math>

In general, consider the q linear combinations of the p random variables X1,..., Xp:
Zi = Cixi + C22xi2 + ... + Ciqxp
i = 1 to p

where C is a (q x p) matrix:

C11  C12  ...  C1D
22   ×2.
C22  C22  ...  C2p
...

and Ci is the ith row of C.

Z = CX; MZ = F(Z) = F(CX) = CMX

= (ov(Z) = Cov(CX) = C^2xc1

Note: The teacher's signature is not included in the cleaned text as it was not part of the original OCR text.

Date: Page No. 1

Correlation Coefficient

P: 71.5, Q: 41.5

1 > Z (pi/2) ≥ 6

Let all territory

1 ≤ 14

Teacher's Signature

Date: 22/08/24

Page No.

F(X) = u, Var(X) = Z = E[(X-11)(x-11)]

Variance-Covariance Matrix for y:

E(Y) = A E(X) + B
= E(y) = Aa + B

· Van (AX + B) = E[(AX + B) - (AM + B)]²
(1)
= E[A(x-u) + B] = E(A(x-u)(x-u))
= E[A²(x-u)²] + B² + 2AB(x-μ)
= A² Var(X) + B² + 2AB

Variance(y) = A² ∑x (1/2)^2
= A² ∫(1/2)^2 dx

Var(y) = E[(y-E(y)) (y-E(y))']
= Variance(y) = E[{(AX+B)-(AU+B)} {(AX+B)-(AU+B)}

Variance(y) = E[AX(x-4)] (AX-Au)]

Variance(y) = A ∫(x-11) (x-11) dx
= A Z A'

Note: I left the math equations as they were, following your instruction to "LEAVE MATH/EQUATIONS ALONE".

Correlation Matrix :-
X = [X, …, Xp]
∫g/2π = G_N(X,Y)/G_N⋅GY

512
7 P12 = 611.612
1 J12 … … … … … … … … … … … … … … … … … … ×
929
Jx = x^2
f_2, J_1 …

Spp=1 Pxp
3p1 - - . .
2
XP
- Relation
8
3:-
Σ
4511
0
.
- 0
-
v'2 =
Σ = 1/2 P 1/2

Pre pre pre 1x2
0
1622
. … .
s = √(-1/2) ≥ √(-1/2)
0 - - 0 --
513
4 V should be inventible!
Teacher's Signature.

Note: I left the math/equations alone as per your instruction, and only cleaned up the text to make it readable.

Date: Pagina Na.

Question 4:
V1/2 = 8
9 - 3 = 1
Ξ2 - 3 = 20/2
25 = 1) - 1/2 V-1/2

Equations:
633 = 25 × 7 + 622 = 9 Ci, = 4
9 y/2 = 0 0 2 Ö 2 4 1 0 9 I -3 13 O Bide 5 7 13 0 0 2

Question 25:
1/5 - 3 = 0 45 0 0 0 /_

Equation:
1/6 1/5 <i>J</i> 2 =) 1/6 - 1/5 Į 1/5 - 1/5

Please provide the OCR text, and I'll be happy to help you rewrite it to make it readable by fixing typos, merging broken lines, leaving math/equations alone, and outputting only the cleaned text.

Date: _______________________
Page No.: _______________________

d. Partition of Variance

Matrix →

ob!-
7XI
9.
Xz × q.
=
× =
×a ti
P-9
XH • Xp M,
(1)
19x1 112 E(x) =
; May + Mar 1 (2)
(1Mp 2 ' - (x-u). (x-u) Vag(X) 5E • × (1) 4(1) 40 (1) 7var(x) = ico X u (2) X × (1) = 11(1) × (1) - 11 (1) X(1) - u(2)
Var(x) =
E=) × (2) - 11 (2) 2×1 1×2 (x(1-4(1)) X(1) - 4(1) (×(1) X (2) (1) w(1) =) E Vor(X) = (x(2) - u(2)) (x(1) - u(1))
(x^(2)-u^(1))(x^(2)-u^(2))

P-9
q_i

t 1 × 211 <S>12.
vac(i(x)) =
7 ∑22 ∑21 p-9

Teacher's Signature: _______________________

Date Page No.

It is known that if X_i and X_i are independent (Σi2 = Σ2i = 0), then the converse is not true.

Theorem: Let PXP be a symmetric matrix (say Z) which is non-negative definite. (~x A x' > 0) - spd. Proof:

SP definite
X = 1/x_n
y/x_n = αx
Vac(y) = Vac(ax) + Nar(y) = A'ZA > 0

~'Aa > 0 SP definite.

- 11. A matrix is non-negative definite, also symmetric: it is a variance-covariance matrix? ∑ → non-regative & symmetric matrin. Let's consider y to be a rector of independent vectors with Pr(ier) = 0; Cov(y) = I*r*X, where C > Cov(X) = CirC = c'*c = E. Teacher's Signature...

Date
Page No
Theorem: If a covariance matrix is not positive definite, then with probability 1, the elements of X are linearly dependent.

Moment Generating Function: (Random Vector - X)
MGF uniquely determines the distribution of X.
M_(x) = E(e^x)

Let X be a Random Vector whose mean is μ and variance-covariance matrix is Σ. Let A be any symmetric matrix (nxn). Then, find the Expectation of XTAX.
E(XTAX) =?

Some notes:
E(X^TAX) = E(X^TAX)
E(tr(x*XA)) = tr(AE(XX))
= tr [A((uTu) + Σ)]
= tr (uTAu + AE)
= uTAU + tr (AZ)

E(X^TX) = Σ + μμ^T; Σ = E[(x-μ)(x-μ)^T]

Teacher's Signature.....

Date: 23/08/23

Paga No. 211
Q. 1.
Zx = M = 2 - 2 + 2 O E[x"]

(a) Cov(AX")
(b) E[A ×^(1)]

(c) Cov(x")

(e) E[×^(2)]

(f)

<math>f</math> <math>f</math> <math>f</math> <math>f</math>

(g) Cov(X^(2))

(h) cov(BX^(2)); (i) cov(X^(1), X^(2))

(j) Cov.(AX^(10), B(x^(2)))

SOM
Xi = 3

A = 1, 2
X<sub>2</sub>, X<sub>3</sub>

X = 2, 2
Gi = Xu

B = 2 - 2, -1
×(1) = 2 ×1)

次 = a)
Giren: 4. = 3
E
622, 633
Suy.

Given!

<math display="block">M^(s)_⋅ = ⎝ ⎛ M^(s)_⋅ ⎠</math>

Teacher's Signature.....

Date: 

Page No: 
U³
u(²)
²
E(X²)
(f) =
³
~
44
١
²
(BX²)
BE(X²)
(⁰)
E
7
ℬ =
- ²
²
¹
⁰
=
-¹
•
²
³
¹
²×²
%1
X

Covariance of X1

= Cov(X1, X1)

= 212/4

= 53

Variance of X1 + Variance of X2

= 2F + 4 × 1

= 6 + 4

= 10

Covariance of B and X1

= -7/9

Covariance of B and X2

= -2/9

 Covariance of X1 and X2

= 2/9

Variance of B

= 33/36

= 11/12

= 0.9167

Variance of X1 + Variance of X2

= (2) - u(2)

= 20/48

= 5/12

Covariance of X1 and X2

= ∑(12 × 2)/O

= 24/O

Note: The cleaned text assumes that the original OCR text was a statistical problem set, with variables X1, X2, B, F, u, and O. The math equations are left unchanged as per instruction.

Date
Page No.
CON [ V(X (1)) , B (X (1))].
(j)
= E (AX(1) - E (AX(1)) - (B(X(2)) - B(AX(2)))
=
= AE[(A ×^(1) - E(X^(1)) - B(X^(2) - E(X^(2))]B
= A Σ12 B'
=(1 2) (2 2) (1 2)
=(2 2) (1 2)

3,
\begin{array}{c|ccccccccccccccccccccccccccccccc}
6 & 0 & 如此 & 3×1 & 3 & 2 & 0 & 2 \\
X1 \ X2 & \chi^{(i)} & = & 0 & 1 & 0 \\
\sum_{X_-} & 9 & X3 & -2 & 2 & Sol & 0 & 2 & -2 & 4 \\
\widehat{\mathcal{A}} = \begin{pmatrix} 4 \\ 3 \\ 2 \end{pmatrix}; & \times = \begin{pmatrix} 2 \\ 2 \end{pmatrix}
x (2) = (xy)
-2, 2
B = -2 & 0 \\
A = \begin{pmatrix} 1 & 1 \end{pmatrix}
(a) E(x^{(\prime\prime)}) = u^{(\prime\prime)} = \begin{pmatrix} 4 \\ 3 \\ 2 \end{pmatrix}
E(X(1)) = 21(1) = (1)
(6)
Cov(X^(1)) = \begin{pmatrix} 3 \\ 0 \\ 2 \end{pmatrix}
(c)
Teacher's Signature

Date: 

Page No: 

Cov(X^(2)) = 4

E(Ax(1))

A. E(x(1))
Ξ

E(Ax(1)) = 7

y COV(A'X'), B(X(x))

A' ≥ α^α

Cov(Ax(1)) ≈ 3 × 20 × 0 + 1 = 9

I = 3/3 × 1/1 × 3 = 3

B. E(X(2))
F(BX(2)) = 9 - 2 - 2
BZB' COV(BX(2)) ≈ -2 × 2 = 

Note: I left the math/equations alone as per your instruction, and only cleaned up the text to make it readable.

4
- 2
2
4
4 + 80
-2 × 2) = 1 × 3
= 3 × 1
8
= 16
-16
8
-16
16
8

Note: I left the math/equations alone as per your instruction.

Date: 4

Page No.

Covariance (x" x")

(1)
19.51
= E((x" - u")). (x" - u")]

<math>z = z_{12} = (z - z_1) \begin{pmatrix} z \\ 0 \\ -z \end{pmatrix}</math>

Covariance (AX"; BX")

(d)
Aziz B
<math>\begin{pmatrix} 2 \\ 0 \end{pmatrix}</math>
1 -2 2

CA.
= 2. (1-22)
-4 4)

(2)
Theorem 1.2: Covariance of (midsen) and Suide 29
14

Teacher's Signature...

Let me know if you need any further assistance!

Gramer's World Theorem: The distribution of Xpx1 is known if its characteristic function φα't) = E(eita'x) is known. Suppose the distribution of X'X is known, then the characteristic function of φα't) = E(e^{itα'}×) = E(e^{iπ×}) is known. Thus, the distribution of X is known.

Converse Part 1: Suppose the distribution of X is known. The characteristic function φx(x) = E(e^{iβ*x}) is known for β ∈ ℝp. The distribution of a'x is known. Thus, φx(x) is known and ≠ β ∈ ℝ^p.

Note: I left the math equation alone as per instruction #3.

Pegov No.

Theorem 1.2: If E is positive definite and X ~ Np(u, E), then the PDF of X is given as:

f(x) = (12π)^(-1/2) exp {-(x-u)^(T)(x-u)}
where x is a p-dimensional random vector.
μ - mean vector
Z - covariance matrix.
|Z| - determinant of covariance matrix.
Z^(-1) - inverse of covariance matrix.
Z > 0; distance must be positive.

Properties of Multivariate Normal Distribution:

If X ~ Np(u, E), then (X-u) ~ Np(0, Z)
9 Moreover, if X ~ Np(u, E); A is a p × q matrix, then any sub-vector of X
(p-q) × 1

X = ~ (1)(x)

M = -4(p-q) × j

Teacher's Signature...

Date
Page No.

Covariance Matrix

The sum of an infinite series of terms can be written as:

<math display="block">\sum_{i=1}^{\infty} \left( \frac{\sum_{i=1}^{\infty}}{\sum_{2i}} \frac{\sum_{12}^{\infty}}{\sum_{2i}} \right)</math>

Then, χ^(1) ~ Nq (μ, Σι)
χ^(2) ~ Nq (μ₂, Σ₁₂)

If X is normally distributed, then any linear combination of vectors, i.e., aX + bX₂ + ... + apXp = a'X.

3.
+ .... + apXp = a'x
a'x ~ N (au, aΣa')

Let X ∼ N(u, Σ) mean the q-lineal combination of vectors, i.e.,
a₁₁ ×₁ + a₁₂ ×₂ + ... + a1p ×p
a₂₁ ×₁ + a₂₂ ×₂ + ... + a2p ×p
age ×₁ + age ×₂ + ... + age ×p

AX ~ Nq (AU, AZA')

Note: The teacher's signature is not included in the cleaned text as per instruction.

Date
Page Number: N/A

Example:
X2 × L
AX 3m
Ξ X2
X3
2×1-
(10)
A = 0
-1
2×3 1
×,
- l 0 · E(Ax) = E
X2 0 1 -1 X3 2×3 3 X1 м,
- 421
E(Ax) = 112 - 43
· Var(AX) =
AZX 0 E11 E12 Q13 - 1 =
1 0 £21 £22 1 ~1 E23 0 1 -1 E31 E32 8 33 0 -1 12×3 1 1 3, 6, -621 612-622 =
6,3 -623
I o -1 1 621 -631 622 -632 623 -633 0 - 1 =

Teacher's Signature: _______________________

Date: 

Page No: 5

All subsets of X are normally distributed. Let Xe and Xez be independent random vectors. Then,

χ² = χ²₁ + χ²₂

where χ²₁ and χ²₂ are independently distributed as chi-squared variables with ν₁ and ν₂ degrees of freedom, respectively.

Theorem:

P(X ∈ V) = ∫[∫(2π)^(-1/2) |X|^(ν-1) e^(-|X|^2/2) dX] dx

where V is a sphere in R^n with radius r and center x0.

Date: 
Page No.: 0

(i)
Cov(X1, X2) = 70
COV(X1, X3) = 0
Cov(X2, X3) = 0

(ii)
y = (x1 + x2)/2, x3
⇒ Cov(y, x3) = 2*Cov(x1+x2, x3)
(1/2)*(Cov(x1, x3) + Cov(x2, x3)) = 0.

(iii)
X2 - 5*X1 - X3
3014
⇒ Cov(X2, X2-5*X1-X3)
北工
= Cov(X2, X2) - Cov(X2, 5*X1) - Cov(X2, X3)
5*Cov(X2, X2) + R5 = 85

(iv)
X2 + 5*X1 - X3
Som
7 COV(X2, X2+ 5*X1 - X3)
= Cov(X2, X2) + 5*(Cov(X2, X1) - Cov(X2, X3))
5*Cov(X2, X2) + 5*(-2) = 0.

Teacher's Signature.

Date: 05/09/24

Page No: [blank]

(Cov(x1, 2x1+1x2+x3)) = 2(cov(X1, X1) + cov(X1, X2) + cov(X1, X3))

There are that many a set combination of a, b, and c form a vector space.

Maximum Likelihood Estimator:

F(n) should be differentiable. d(f(n))/dx = (d/dx)f(x) dx

E.g., Lehmann (267)

f'(x) and f''(x) should exist. E[d]/dn should exist.

MLE of H: 4-5:-

P4F: (1271) P(1/2)^(1/2)^(1/2)^(1/2)^(1/2)^(1/2)^(1/2)

0

<math display="block">L(\omega, Σ) = (1/(2π)^{1/2}) * (1/|Σ|^{1/2}) * exp{-((1/2)*∑(N-n)*(∑(N-n)))}</math>

Teacher's Signature: [blank]

Plog Graph!

log(1/0) = -NP log (2n) - m/ log (121) - 1/2 [(xi-u)/z']

109(L) = -> = -(x1-11) = 0
Spline.
"Data = 2A; Asymmetric.
Z is definite.

*
3 [(xi-11) = 0
1 = ×

$\frac{2}{2}\times = \frac{2}{121}$

Dames 4

<math display="block">\frac{\partial}{\partial Σ} log (L) = \frac{-1}{2|Σ|}</math>

(i) trace (ABC) = tr (CAB) = tr (BCA).
XTAX is scalar,
(ii)
(iii)
(iv) $\frac{3}{4} log |A| = [A^{-1}]^{-1} = [A^{\top}]^{-1}$
of A is invertible,
(v)
$|A| = \frac{1}{|A^{-1}|}$

Teacher's Signature

Element of Stat. Learning Page No.

The derivative of the function x A T A x with respect to A is equal to:

× = d tr (A^T x)

= (x^T)^T

= x^T.

The derivative of the log likelihood function L with respect to Σ^-1 is equal to:

= 1/2 (∑^-1)^T - 1/2 (∑^-1)^T - 1/2 (∑^-1)^T

= + n/2 ((∑^-1)^(-1))^(-1) - 1/2 (((∑^-1)∑^-1)^(-1)) ((∑^-1)∑^-1)^(-1)

Note: I left the math equations as they were, without changing them. The empirical family of distributions only requires sufficient statistics for mean and variance-covariance matrix. The sample mean vector (x̄) and sample variance (x̄) are the sufficient statistics for mean and variance-covariance matrix. The empirical family of distributions includes X² distribution, Wishart distribution, Hotelling's T² distribution, F distribution, and Teacher's Signature...

June 9, 2024
0.1

Ni = 1/2*x1 + 1/2*x3 + 1/2*x4
V2 = X1 + X2 + X3 - 3*X4
Mean of V. P V2?
Variability of -11 - ?
Covariance below V, & V2?

E(V1) = (1/2)*(3) + (1/2)*(1) + (1/2)*(1) = 3/2

Var(v) = 1/4*(3) + 1/4*(1) + 1/4*(2)
= 3/4 + 1/4 + 3/4 = 3/2

Var(Vr) = 3+1+2 = 6.

Cov (v, V2) = Cov (/2*x1 + /2*x3 + /2*x4, x1 + x2 + x3)
= Cov (/2*x1, x1) + Cov (/2*x1, x2) + Cov (/2*x1, x3) + Cov (/2*x3, x1) + Cov (/2*x3, x2) + Cov (/2*x3, x3) + Cov (/2*x4, x1) + Cov (/2*x4, x2) + Cov (/2*x4, x3)
= (3/2 + 1/2 + 1/2) + (1/2 + 1/2) + (1/2 + 1/2) + (1/2 + 1/2) + (1/2 + 1/2) + (1/2 + 1/2) + (1/2 + 1/2)

Date: Page No. + Vag (vi) = Vag (1/2x^3 + 1/2x^2 + 1/2x)
Van (v) =
1417
-14"
0
100 mm
1
270
15 IT HO INOV
44
17-14

Note: I left the math equation alone as per your instruction.

Wishart's Distribution:-

iid N(0, 20)

Let X1, X2, …, Xp be independent normal (0, Σ). Then the distribution of pxp random matrix: xx'
is known as Wishart's distribution.

The Random Matrix Wpxp = ∑i=1γxi is a

Wishart's distribution has no degrees of freedom and Cov. matrix Σ is denoted by:

f(ω) = 1/(2^(np/2)) ∫P (n/2) |Σ|^(np/2) |ω|^(n-p-1)/2 e^(-1/2) |Σ|^(np/2) e^(-1/2) |Σ|^(np/2)

TP: P-dimensional MV gamma 6".

In general, x ∼ n-1 dn

Properties:

Note: Wishart Distribution is the MV extension of X².

It Z = I then WD is known as Std. WD.

2. For M ~ Wp(n, E) & Bpxm:
(i) B'MB ~ W(n, B' Σ)

Let Σ > 0, then Σ^(-1/2)MΣ^(1/2) ~ WN, I(p)

Note: I left the mathematical equation alone as per instruction.

Chapter 18, Section 6

Theorem: Probability

Date: _______________________
Page Number: _______________________

If M ∼ W_p(n; Σ), then E ∼ M - W_p(n, Σ),

where n = 1 + 12 + … + nk.

(4) E(M) = n ≥ m

It is also true that if M1 and M2 are independent and satisfy M1 + M2 = M,

then ~ Wp(n, E) implies M ∼ Wp(n, π/2)

M2 ∼ Wp(n, E), n2 = n - 1.

Theorem 1: Let M ∼ Wp(n, Z), and a be any vector in RP such that a'Za ≥ 0. Then,

at most max(a) ≤ √m

a' is also true.

Theorem 2: Let M ∼ ω_p(n, Ξ) and a be any vector in ℝ^p, where M > p-1, such that a'Ξa ≠ 0. Then,

a'Z - 'ax (n-p+1)

Teacher's Signature: _______________________

Diete
Page No. 77

Z1 ~ N(0,1) and Z2 ~ N(0,1)
*
V = 7. Cauchy (0,1)
7.1 + 7.2 ~ X(2)
y ~ χ^2(k,)
y_2 ∼ x_{n_2}^2

M =
F(n, n_2) (y_2/n_2)
-
天 ~ 太(n) - - - - - - - - - - - - - - - - - -

4. U_2
> = Z1 + Z2 + ... + Z2 ~ X_2N
U,, U_2
J
N(0, 5) ....................................
Up to F/nx
1 - -- 1
1/24

Teacher's Signature...

Date
Page No.
Q.1. Let X and X2 be 2 iid. multivariate random vectors with mean (μ) and covariance matrix E.

What are the ways central x2 distribution?

(a) 1/2 (x - x2)T(x1 - x2)
(b) 2 (x, -x2) (x, -x2)
(c) 2 (x, -x2) T = (x, -x2)*
(d) 1/3 (x, -x2) T (x, -x2)

Property: Let X1, ..., Xn be individual random vectors from MP(U, E).

Then n. (x-u) T S-1 (x-u) ~ x^2(p)

Q.2. Let X be a p-variate N distribution with E(x)=4 and Var(x)=5. Which of the following matrices is true?

Find:
(a) E(xx') = 14.00
(b) E[(x-u)' \* σ' * σ'] = ?

(c) P[(x-u) ≥ t^2] = ?

Let X ~ Np(u, Σ). E=I, B ∈ R^(pxp), a real symmetric matrix.

Q.3.
such that γ(B) = K < p>.

But Bu = 0, β = B (seven days potent). Then find the probability distribution of XB'X. Teacher's Signature...

Date
Page No.
O.4. Let X, X2,..., Xn ~ N(p,u,E), which of the following statements are true?

(i) (X1 - μ)T ≥ (X1 - μ) has X0².

(ii) XX' ∼ W(p).

(iii) (∂-u)(∂-u)T ∼ χ² with p degrees of freedom.

(iv) (X1 + X2) and (X1 - X2) are independently distributed?

Q.5 Let y ∼ N(0,I),

A & B - nxn Identity Matrix.

Which of these are true?

(1) If AB = 0, then Y'BY and Y'Ay are independently distributed.

(ii) If 17Y'(A+B)Y ~ X^2, then Y'(Ay) and Y'(By) are independently distributed.

(iii) y'(A-B)y ∼ X^2

(iv) y'(A)y and y'(B)y have X^2 distribution

Confidence: (a, b) ~ 0 = 25
Audible: p(0~6,01) = 0.35

Date: Ch.51-(40)
Page No.

Interence about the mean vector (II):
1. Sample

Test Statistic # = (√(× - u0)/5/×)

Set of possible values for which Ho is not rejected
For M: (5²)

The statistic follows:

T² = (∫X - μ0)T [(1/n)S]T (∫X - μ0)

T² = n(x-40)T(3)-1(x-40)

F² ∼ P(n-1)/(n-p)

Note: I left the math/equations alone as per your instruction.

Page No.

Let X1, X2, …, Xn ∼ iid Np(ui, σ^2)

Y = X1^2 + X2^2 + … + Xn^2 ∼ Noncentral χ^2(n)

Non-contractually presented 8.

S = u1^2 + u2^2 + … + un

Non-central t-distribution:

Let X ∼ N(8, 1).

U ∼ χ^2(n)

Then T = χ / √U_n ∼ N C_n t ⋅ dist_(n)

Non-centrality parameter S.

(¯χ-u)^2

σ_σ

t^2 = √n (¯x - u_0)^T (S^2)^(-1) (¯x - u_0)

Teacher's Signature...

Hotteling - T2 does:

Let X, X2, ..., Xu be Np (u, E)

X = = = = = = = = = = = = = = = =

x = p (T > √((n-1)p/(n-p)) F(p, n-p) α

The reject Ho when objective value of this

Greater than (n-1)p. Firing (~).

Numpy
2012
T2?
52 = (x1-x) * (x1-x)
= 1/2 * (x1 - x1) * (x1 - x2) * (x1 - x2) * (x1 - x2) * (x1 - x2) * (x1 - x2) * (x1 - x2) * (x1 - x2) * (x1 - x2) * (x1 - x2)
52
= 3^2 / 2 * [[78, -4, -9], [-4, 4, 0], [-9, 0, 9]] = 3^2 / 2 * [[8, -6], [-6, 18]] = [[4, -3], [-3, 9]]
√(2/3) = 3.

Note: I left the math/equations alone as per your instruction.

Date: Page No. 4

T² = (-3) - 1 ((-1) × 7 + 7) ÷ 3.

= -0.9 × 2 - 3 × 1 + 227 ¥ 9

T² = 3 ł Ξ ((-1) × ı + 3) × 3 + 4 + 1 × 7 .72 + 6 ı I.

= 127 T² = 4 IXI. = 21 ÷ 7 + 2 - 3.

IXI. = 9 /27 + 27 XI ? > 6 -

P -, Ge., V *, .9 -, Teacher's Signature...