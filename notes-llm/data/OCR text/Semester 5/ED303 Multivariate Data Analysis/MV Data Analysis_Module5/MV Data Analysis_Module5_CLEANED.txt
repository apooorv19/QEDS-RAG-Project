The Multivariate Normal Distribution
3.1 Introduction

A generalization of the familiar bell-shaped normal density to several

Dimensions plays a fundamental role in multivariate analysis. While real data are never exactly multivariate normal, the normal density is often a useful approximation to the "true" population distribution because of a central limit effect.

One advantage of the multivariate normal distribution stems from the fact that it is mathematically tractable and nice results can be obtained.

To summarize, many real-world problems fall naturally within the framework of normal theory. The importance of the normal distribution rests on its dual role as both population model for certain natural phenomena and approximate sampling distribution for many statistics.

The Multivariate Normal Density and Its Properties

• Recall that the univariate normal distribution, with mean μ and variance σ^2,
has the probability density function
f(x) = 1/√(2πσ^2) e^[-((x-μ)/σ)^2/2] -∞ < x < ∞
• The term
((x-μ)/σ)^2 = (x-μ)(σ^2)^(-1)(x-μ)
• This can be generalized for p × 1 vector x of observations on several variables
as
(x - μ)' Σ^(-1) (x - μ)
The p × 1 vector μ represents the expected value of the random vector X,
and the p × p matrix Σ is the variance-covariance matrix of X.

A p-dimensional normal density for the random vector X' = [X_1, X_2, …, X_p] has the form

f(x) = (1/(2π)^{p/2}|\Σ|^{1/2})e^(-(x-μ)'Σ^{-1}(x-μ)/2}

where -∞ < x_i < ∞, i = 1, 2, …, p. We should denote this p-dimensional normal density by N_p(μ, Σ).

Figure 4.1 A normal density with mean μ and variance σ^2

954

and selected areas under the curve.

μ - 2σ    μ - σ
u + σ
μ + 2σ

Example 3.1 (Bivariate Normal Density)

Let us evaluate the p=2 variate normal density in terms of the individual parameters μ1 = E(X1), μ2 = E(X2), σ11 = Var(X1), σ22 = Var(X2), and ρ12 = Corr(X1, X2).

Result 3.1

If Σ is positive definite, so that Σ^{-1} exists, then Σe = λe implies Σ^{-1}e = (1/Λ)e, so (λ, e) is an eigenvalue-eigenvector pair for Σ corresponding to the pair (1/λ, e) for Σ^{-1}. Also Σ^{-1} is positive definite.

Figure 4.2: Two bivariate normal distributions. (a) f(x1, x2), x1. (b) σ11 = σ22 and ρ12 = 0. (b) σ11 = σ22 and ρ12 = 0.75.

Constant probability density contour equals { all x such that (x - μ)' Σ^(-1) (x - μ) = c^2 } = surface of an ellipsoid centered at μ. Contours of constant density for the p-dimensional normal distribution are ellipsoids defined by x such that (x - μ)' Σ^(-1) (x - μ) = c^2 These ellipsoids are centered at μ and have axes ± c√λi ei, where Σei=λi for i = 1, 2, …, p.

Example 4.2: Contours of the bivariate normal density
Obtain the axes of constant probability density contours for a bivariate normal distribution when σ11 = σ22.

Figure 4.3: A constant-density contour for a bivariate normal distribution with σ11 = σ22 and x1 ~ c√σ11 + σ12 - σ12 + μ2.
This contour is shown in Figure 4.3, where ρ12 > 0 (or σ12 > 0).

The solid ellipsoid of x values satisfying (x - μ)' Σ^{-1} (x - μ) ≤ χ_p^2(α)
has probability 1-α where χ^2_p(α) is the upper (100α)th percentile of a chi-square
distribution with p degrees of freedom.
μ_2 → x_1
μ_1
Figure 4.4 The 50% and 90% contours for the bivariate normal distributions in Figure 4.2.

Additional Properties of the Multivariate Normal Distribution

The following are true for a normal vector X having a multivariate normal distribution:

1. Linear combinations of the components of X are normally distributed.
2. All subsets of the components of X have a (multivariate) normal distribution.
3. Zero covariance implies that the corresponding components are independently distributed.
4. The conditional distributions of the components are normal.

Let me know if you need any further assistance!

Result 3.2
If X is distributed as N_p(mu, Sigma), then any linear combination of variables a'X = a_1X_1 + a_2X_2 + ... + a_pX_p is distributed as N(a'mu, a'Sigmaa). Also if a'X is distributed as N(a'mu,a'Sigmaa) for every a, then X must be N_p(mu,Sigma).

Example 3.3 (The distribution of a linear combination of the components of a normal random vector)
Consider the linear combination a' X of a multivariate normal random vector determined by Choice: $\mathbf{a}' = [1, 0, \dots, 0]$. Result 3.3 If $X$ is distributed as $N_p(\mu, \Sigma)$, the $q$ linear combinations $\mathbf{A}_{(q \times p)} \mathbf{X}_{p \times 1} = \begin{bmatrix} a_{11} X_1 + \dots + a_{1p} X_p \\ a_{21} X_1 + \dots + a_{2p} X_p \\ \vdots \\ a_{q1} X_1 + \dots + a_{qp} X_p \end{bmatrix}$ are distributed as $N_q(\mathbf{A}\boldsymbol{\mu}, \mathbf{A}\boldsymbol{\Sigma}\mathbf{A}')$. Also $\boldsymbol{X}_{p\times 1} + \mathbf{d}_{p\times 1}$, where $\mathbf{d}$ is a vector of constants, is distributed as $N_p(\mu + \mathbf{d}, \Sigma)$.

Example 3.4 (The distribution of two linear combinations of the components of a normal random vector)

For $\boldsymbol{X}$ distributed as $N_3(\boldsymbol{\mu}, \Sigma)$, find the distribution of
$\begin{bmatrix} X_1 - X_2 \\ X_2 - X_3 \end{bmatrix} = \begin{bmatrix} 1 & -1 & 0 \\ 0 & 1 & -1 \end{bmatrix} \begin{vmatrix} X_1 \\ X_2 \\ X_3 \end{vmatrix} = \mathbf{A}\mathbf{X}$

Result 3.4 All subsets of X are normally distributed. If we respectively partition X, its mean vector μ, and its covariance matrix Σ as X(p×1) = | | X_1 | | (q×1) | | ... | | X_2 | | (n-a)×1 | | | μ(p×1) = | | μ_1 | | (q×1) | | ... | | μ_2 | | (p-q)×1 | | | and Σ(p×p) = [ Σ_{11} & Σ_{12} | | (q×1) & (q×(p-q)) | | ... | | Σ_{21} & Σ_{22} | | ((p-q)×q) & ((p-q)×(p-q)) | | ] then X_1 is distributed as The distribution of a subset of a normal random vector. If X is distributed as N(μ, Σ), find the distribution of [X2, X4]'. Example 3.5

Result 3.5 (a) If m X_1 and m X_2 are independent, then m Cov(m X_1,m X_2)=0, a q_1 x q_2 matrix of zeros, where m X_1 is q_1 x 1 random vector and m X_2 is q_2 x 1 random vector.

(b) If |X_1 |X_2| is N_(q_1+q_2)( |μ_1 |μ_2|, |Σ_11 Σ_12 Σ_21 Σ_22| ), then X_1 and X_2 are independent if and only if Σ_12 = Σ_21 = 0.

(c) If m{X}_1 and <math>mX_2</math> are independent and are distributed as <math>N(q_1)(mμ_1,Σ11)</math> and <math>N(q_2)(μ_2, Σ22)</math>, respectively, then <math>|{array}{c} X_1 \\ X_2 |{array}</math> has the multivariate normal distribution <math>N(q_1+q_2)|{[array}{c|c} μ_1 & , & Σ11 & 0 \\ μ_2 & , & 0 & Σ22 |{array}]}</math>.

Example 3.6: The Equivalence of Zero Covariance and Independence for Normal Variables

Let X be N(μ, Σ) with Σ = [[4, 1, 0], [1, 3, 0], [0, 0, 2]] Are X1 and X2 independent? What about (X1, X2) and X3?

Result 3.6: Let X be distributed as Np(μ, Σ) with μ = [[μ1], [μ2]], Σ = [[Σ11, Σ12], [Σ21, Σ22]], and |Σ22| > 0. Then the The conditional distribution of $m{X}_1$, given that $\oldsymbol{X}_2 = \oldsymbol{\mathsf{x}}_2$ is normal and has Mean = $\mu_1 + \Sigma_{12}\Sigma_{22}^{-1}(\mathbf{x}_2 - \mu_2)$ and Covariance = $\Sigma_{11} - \Sigma_{12}\Sigma_{22}^{-1}\Sigma_{21}$ Note that the covariance does not depend on the value $\mathbf{x}_2$ of the conditioning variable.

Example 3.7: The Conditional Density of a Bivariate Normal Distribution

Obtain the conditional density of X1, given that X2 = x2 for any bivariate distribution.

Result 3.7: Let X be distributed as Np(μ, Σ) with |Σ| > 0. Then

(a) (X - μ)'Σ^{-1}(X - μ) is distributed as χp^2, where χp^2 denotes the chi-square distribution with p degrees of freedom.

(b) The Np(μ, Σ) distribution assigns probability 1-α to the solid ellipsoid {x: (x-μ)'Σ^{-1}(x-μ) ≤ χp^2(α)}, where χp^2(α) denotes the upper (100α)th percentile of the χ^2_p distribution.

Result 3.8 Let X1, X2, ..., Xn be mutually independent with Xj distributed as Np(μj, Σ). (Note that each Xj has the same covariance matrix Σ.) Then V1 = c1X1 + c2X2 + ... + cnXn is distributed as Np(∑j=1n cj μj, (∑j=1n cj^2)Σ). Moreover, V1 and V2=b1X1+b2X2+...+bnXn are jointly multivariate normal with covariance matrix (Σc^Tb) Σ. Consequently, V1 and V2 are independent if b'Tc = ∑j=1ncjbj = 0.

Example 3.8 (Linear combinations of random vectors)

Let $\mathbf{X}_1, \mathbf{X}_2, \mathbf{X}_3$ and $\mathbf{X}_4$ be independent and identically distributed $3 \times 1$ random vectors with
$\mu = \left[\begin{array}{c|c} 3 \\ -1 \\ 1 \end{array}\right]$ and $\Sigma = \left[\begin{array}{c|c} 3 & -1 & 1 \\ -1 & 1 & 0 \\ 1 & 0 & 2 \end{array}\right]$

(a) Find the mean and variance of the linear combination $\mathbf{a}' \mathbf{X}_1$ of the three components of $\mathbf{X}_1$, where $\mathbf{a} = [a_1, a_2, a_3]'$.

(b) Consider two linear combinations of random vectors:

$\frac{1}{2}\mathbf{X}_1 + \frac{1}{2}\mathbf{X}_2 + \frac{1}{2}\mathbf{X}_3 + \frac{1}{2}\mathbf{X}_4$

and

$\mathbf{X}_1 + \mathbf{X}_2 + \mathbf{X}_3 - 3\mathbf{X}_4$

Find the mean vector and covariance matrix for each linear combination of vectors, and also the covariance between them.

3.3 Sampling from a Multivariate Normal Distribution and Maximum Likelihood Estimation

The Multivariate Normal Likelihood

• Joint density function of all p × 1 observed random vectors X1, X2, …, Xn
= ∏j=1n { (1/(2π)p/2 |Σ|1/2) e^-(xj - μ)' Σ^-1 (xj - μ)/2 }
= 1/(2π)np/2 |Σ|n/2 e^-∑j=1n(Xj-μ)' Σ^-1(Xj-μ)/2 The probability density function of a multivariate normal distribution with mean vector μ and covariance matrix Σ is given by:

1/(2π)^{np/2} |Σ|^{n/2} e^(-tr[Σ^(-1) (∑(x_j - ¯x)(x_j - ¯x)' + n (¯x - μ)(¯x - μ)')]/2)

where x_1, ..., x_n are the individual data points, ¯x is the mean vector, and Σ^(-1) is the inverse of the covariance matrix.

Likelihood

When the numerical values of the observations become available, they may be substituted for x_i in the equation above. The resulting expression, now considered as a function of μ and Σ for the fixed set of observations x_1, x_2, ..., x_n, is called the likelihood.

Maximum likelihood estimation

One meaning of "best" is to select the parameter values that maximize the joint density evaluated at the observations. This technique is called maximum likelihood estimation, and the maximizing parameter values are called maximum likelihood estimates.

Let A be a k × k symmetric matrix and x be a k × 1 vector. Then

(a) x'Ax = tr(x'Ax) = tr(Axx')

(b) tr(A) = ∑i=1^n λi, where the λi are the eigenvalues of A.

Maximum Likelihood Estimate of μ and Σ Result 3.10 Given a p × p symmetric positive definite matrix B and a scalar b > 0, it follows that 

1/(Σ^b) e^(-tr(Σ^-1B)/2) ≤ 1/|B|^b (2b)^p b e^(-bp)

for all positive definite Σ_{p×p}, with equality holding only for Σ=(1/2b)B. Result 3.11 Let X_1, X_2, …, X_n be a random sample from a normal population with mean μ and covariance Σ. Then μ̂ = X̄ and Σ̂ = (1/n) ∑_{j=1}^n (X_j - X̄) The maximum likelihood estimators of μ and Σ, respectively, are (Xj - X)' = n-1/n S. Their observed values x̄ and (1/n)∑(xj - x̄)(xj - x̄)', are called the maximum likelihood estimates of μ and Σ.

Invariance Property of Maximum Likelihood Estimators
Let $\hat{\theta}$ be the maximum likelihood estimator of $\theta$, and consider the parameter $h(\theta)$, which is a function of $\theta$. Then the maximum likelihood estimate of $h(\boldsymbol{\theta})$ is given by $h(\hat{\boldsymbol{\theta}})$. For example:

1. The maximum likelihood estimator of $\mu'\Sigma^{-1}\mu$ is $\hat{\mu}\hat{\Sigma}^{-1}\hat{\mu}$, where $\hat{\mu}=\bar{X}$ and $\hat{\Sigma} = \frac{n-1}{n} \mathbf{S}$ are the maximum likelihood estimators of $\boldsymbol{\mu}$ and $\Sigma$ respectively.

2. The maximum likelihood estimator of $\sqrt{\sigma_{ii}}$ is The maximum likelihood estimator of σii, where σii is the variance of Xi, is given by √σii, where σii = (1/n) ∑(i=1 to n) (Xi - Xī)^2.

Sufficient Statistics

Let X1, X2, …, Xn be a random sample from a multivariate normal population with mean μ and covariance Σ. Then bar{X} and S=frac{1}{n-1}∑j=1n (Xj-bar{X})(Xj-bar{X})' are sufficient statistics

• The importance of sufficient statistics for normal populations is that all of the information about μ and Σ in the data matrix X is contained in X and S, regardless of the sample size n.
• This generally is not true for nonnormal populations.

Since many multivariate techniques begin with sample means and covariances,

It is prudent to check on the adequacy of the multivariate normal assumption. If the data cannot be regarded as multivariate normal, techniques that depend solely on X and S may be ignoring other useful sample information.

3.4 The Sampling Distribution of X and S

The univariate case (p = 1)

X is normal with mean μ = (population mean) and variance
σ² / n = population variance / sample size

For the sample variance, recall that (n-1)s² = ∑(Xj - ¯X)² is distributed as σ² times a chi-square variable having n-1 degrees of freedom (d.f.).

The chi-square is the distribution of a sum squares of independent standard normal random variables. That is, (n-1)s² is distributed as σ²(Z₁² + ⋯ + Zₙ₋₁²) = (√σZ₁)² + ⋯ + (√σZₙ₋₁)². The individual terms √σZi are independently distributed as N(0, σ²).

Wishart distribution
W_m(·|Σ) = Wishart distribution with m d.f.
= distribution of ∑_{j=1}^{n} Z_j Z_j'
where Z_i are each independently distributed as N_p(0,Σ).

Please provide the OCR text, and I'll be happy to help you rewrite it to make it readable by fixing typos, merging broken lines, leaving math/equations alone, and outputting only the cleaned text.

If $\mathbf{A}_1$ is distributed as $W_{m_1}(\mathbf{A}_1|\Sigma)$ independently of $\mathbf{A}_2$, which is distributed as $W_{m_2}(\mathbf{A}_2|\Sigma)$, then $\mathbf{A}_1 + \mathbf{A}_2$ is distributed as $W_{m_1+m_2}(\mathbf{A}_1 + \mathbf{A}_2|\Sigma)$. That is, the degree of freedom add.

If $\mathbf{A}$ is distributed as $W_m(\mathbf{A}|\Sigma)$, then $\mathbf{C}\mathbf{A}\mathbf{C}'$ is distributed as $W_m(\mathbf{CAC'}|\mathbf{C}\Sigma\mathbf{C}')$.

The Sampling Distribution of $\bar{X}$ and ${\bf S}$
Let $X_1, X_2, \ldots, X_n$ be a random sample size n from a p-variate normal distribution with mean $\mu$ and covariance matrix $\Sigma$. Then
1. $\bar{\boldsymbol{X}}$ is distributed as $N_p(\boldsymbol{\mu}, \frac{1}{n}\Sigma)$.
2. $(n-1)\mathbf{S}$ is distributed as a Wishart random matrix with $n-1$ d.f.
3. $X$ and $S$ are independent.

Note: I left the math/equations alone, as per your instruction.

4.5 Large-Sample Behavior of X and S

Result 3.12 (Law of Large Numbers)

Let Y1, Y2, ..., Yn be independent observations from a population with mean E(Yi) = μ, then

¯Y = (Y1 + Y2 + … + Yn)/n

converges in probability to μ as n increases without bound. That is, for any prescribed accuracy ε > 0, P[−ε < ¯Y − μ < ε] approaches unity as n → ∞.

Result 3.13 (The Central Limit Theorem)

Let X1, X2, ..., Xn be independent observations from any population with mean μ and finite covariance Σ. Then

√n(¯X−μ)

has an approximate Np(0,Σ) distribution for large sample sizes. Here n should also be large relative to p.

Large-Sample Behavior of $\bar{X}$ and ${\bf S}$
Let $X_1, X_2, \ldots, X_n$ be independent observations from a population with mean
$\mu$ and finite (nonsingular) covariance $\Sigma$. Then
$\sqrt{n}(\bar{\boldsymbol{X}}-\boldsymbol{\mu})$ is approximately $N_p(0,\Sigma)$
and
$n(\bar{\boldsymbol{X}}-\boldsymbol{\mu})'{\bf S}^{-1}(\bar{\boldsymbol{X}}-\boldsymbol{\mu})$ is approximately $\chi_n^2$
for $n-p$ large.

3.6 Assessing the Assumption of Normality

The assumption of normality is a fundamental concept in statistical inference, and it plays a crucial role in many statistical procedures. However, this assumption is not always met in real-world data, and therefore, it is essential to assess whether the assumption of normality holds true for a given dataset.

There are several methods that can be used to assess the assumption of normality, including visual inspections, statistical tests, and graphical techniques. One common method is to use a Q-Q plot, which plots the quantiles of the data against the quantiles of a standard normal distribution. If the points on the plot fall close to a straight line, it suggests that the data are normally distributed.

Another method is to use a Shapiro-Wilk test, which is a statistical test that compares the sample data to a theoretical normal distribution. The test produces a p-value, and if the p-value is greater than a certain significance level (usually 0.05), it suggests that the assumption of normality is not rejected.

In addition to these methods, there are also several graphical techniques that can be used to assess the assumption of normality. For example, a histogram or a box plot can be used to visualize the distribution of the data and identify any deviations from normality.

It is important to note that assessing the assumption of normality is not a one-time task, but rather an ongoing process that should be performed throughout the analysis. This is because the assumption of normality may not hold true for all parts of the dataset, and therefore, it is essential to continuously monitor the data and adjust the analysis accordingly.

In conclusion, assessing the assumption of normality is a critical step in statistical inference, and it requires the use of multiple methods and techniques. By using visual inspections, statistical tests, and graphical techniques, researchers can gain a better understanding of whether their data are normally distributed or not, and make informed decisions about how to proceed with their analysis.

Most of the statistical techniques discussed assume that each vector

Observation X_i comes from a multivariate normal distribution. In situations where the sample size is large and the techniques dependent solely on the behavior of μ{X}, or distances involve μ{X} of the form n(μ'{S}(¯{X}-μ)), the assumption of normality for the individual observations is less crucial. But to some degree, the quality of inferences made by these methods depends on how closely the true parent population resembles the multivariate normal form.

Therefore, we address these questions: 
1. Do the marginal distributions of the elements of X appear to be normal? What about a few linear combinations of the components Xj?
2. Do the scatter plots of observations on different characteristics give the elliptical appearance expected from a normal population?
3. Are there any "wild" observations that should be checked for accuracy?

Evaluating the Normality of the Univariate Marginal Distributions
• Dot diagrams for smaller n and histogram for n > 25 or so help reveal situations where one tail of a univariate distribution is much longer than the other.
• If the histogram for a variable X_i appears reasonably symmetric, we can check further by counting the number of observations in certain intervals. For example, a univariate normal distribution assigns probability 0.683 to the interval (μ_i - √σ_{ii}, μ_i + √σ_{ii}) and probability 0.954 to the interval (μ_i - 2√σ_{ii}, μ_i + 2√σ_{ii}). Consequently, with a large sample size n, the observed proportion The probability, denoted by $\hat{p}_{i1}$, of the observations lying in the interval $(\bar{x}_i - \sqrt{s_{ii}}, \bar{x}_i + \sqrt{s_{ii}})$ to be about 0.683, and the interval $(\bar{x}_i - 2\sqrt{s_{ii}}, \bar{x}_i + 2\sqrt{s_{ii}})$ to be about 0.9543.

Using the normal approximation to the sampling of $\hat{p}_i$, observe that either

$|\hat{p}_{i1} - 0.683| > 3\sqrt{\frac{(0.683)(0.317)}{n}} = \frac{1.396}{\sqrt{n}}$

or

$|\hat{p}_{i2} - 0.954| > 3\sqrt{\frac{(0.954)(0.046)}{n}} = \frac{0.628}{\sqrt{n}}$

would indicate departures from an assumed normal distribution for the $i$th characteristic.

Plots are always useful devices in any data analysis. Special plots called

Q-Q plots can be used to assess the assumption of normality. Let x_(1) ≤ x_(2) ≤ … ≤ x_(n) represent these observations after they are ordered according to magnitude. For a standard normal distribution, the quantiles q_(i) are defined by the relation P[Z ≤ q_(j)] = ∫−∞q(j) 1/√(2π) e^(-z^2/2) dz = p_(j) = (j - 1/2)/n Here p_(j) is the probability of getting a value less than or equal to q_(j) in a single drawing from a standard normal population. The idea is to look at the pairs of quantiles (q_(j), x_(j)) with the same associated cumulative probability (j - 1/2)/n. If the data arise from a normal population, the pairs (q_(j), x_(j)) will be approximately linear related, since σq_(j) + μ is nearly expected sample quantile.

Example 3.9 (Constructing a Q-Q plot)

A sample of n=10 observations gives the values in the following table: Ordered Probability levels Standard normal observations quantiles qi xi -1.645 -1.00 .05 -1.036 -.10 .15 .25 -.674 .16 .35 -.385 .41 .45 -.125 .62 .55 .125 .80 .65 .385 1.26 .75 .674 1.54 .85 1.036 1.71

A Q–Q plot for the data in Example 4.9 is shown in Figure 4.5.

The steps leading to a Q-Q plot are as follows:

1. Order the original observations to get x(1), x(2), ..., x(n) and their corresponding probability values Calculate the standard quantiles q1, q2, ..., qn and plot the pairs of observations (q1, x1), (q2, x2), ..., (qn, xn), and examine the "straightness" of the outcome.

Example 4.10 (A Q-Q plot for radiation data)

The quality-control department of a manufacturer of microwave ovens is required by the federal government to monitor the amount of radiation emitted when the doors of the ovens are closed. Observations of the radiation emitted through closed doors of n=42 randomly selected ovens were made. The data are listed in the following table.

Table 4.1 Radiation Data (Door Closed)

Oven no. 31: .15, .10
16: .10, 2: .09, 17: .02
32: .20, 33: .10, 3: .18, 11: .07, .10, .05, .02
4: .10, 19: , 35: .02, 5: .05, 20: .40, 36:
21: .10, .20, 6: .12, 22: , 37: .20, 7: .08, .05, .30
23: .03, 38: , 8: .05, 39: .30, 24: , 9: .08, .05, 40: .40, .10
25: .15, 10: .30, 26: , 41: , 11: .07, .10, .05, .02
27: .15, 42: , 12: , 28: .09, 13: .01, 29: , 14: .10, .08
15: .10, 30: .18

Source: Data courtesy Figure 4.6: A O-O plot of the radiation data (door closed) from Example 4.10.

The integers in the plot indicate the number of points occupying the location, with values ranging from -2.0 to 3.0, including -1.0 and 0.

The straightness of the Q-Q plot can be measured by calculating the correlation coefficient of the points in the plot. The correlation coefficient for the Q-Q plot is defined as:

r_Q = (∑(x_(j) - x̄)(q_(j) - q̄)) / (√(∑(x_(j) - x̄)^2) √(∑(q_(j) - q̄)^2))

and a powerful test of normality can be based on it. Formally, we reject the hypothesis of normality at a level of significance α if r_Q falls below the appropriate value in the following table:

<b>Table 4.2</b> Critical Points for the O–O Plot

Correlation Coefficient Test for Normality

Significance levels α

Sample size | .01 | .05 | .10
---------|-----|-----|-----
n        | 5   | .8299 | .8788 | .9032
         |     | .8801 | .9198 | .9351
10       |     | .9503
15       |     | .9126 | .9389 | .9604
20       |     | .9269 | .9508 | .9665
25       |     | .9410 | .9591 | .9652 | .9715
30       |     | .9479 | .9538 | .9682 | .9740 | .9726 | .9771 | .9599
40       |     | .9749 | .9792 | .9632
45       |     | .9768 | .9809 | .9671
50       |     | .9822 | .9787
55       |     | .9695 | .9836 | .9801
60       |     | .9720
75       |     | .9838 | .9866 | .9771 | .9873 | .9895
100      |     | .9822 | .9879 | .9913
150      |     | .9928
200      |     | .9931 | .9942 | .9905 | .9953 | .9960
300      |     | .9935

Example 3.11 (A correlation coefficient test for normality) Let us calculate the correlation coefficient r_Q from Q-Q plot of Example 3.9 and test for normality.
37

Linear combinations of more than one characteristic can be investigated. Many statisticians suggest plotting $\hat{\mathbf{e}}_1'\mathbf{x}_i$ where $\mathbf{S}\hat{\mathbf{e}_1} = \hat{\lambda}_1\hat{\mathbf{e}_1}$ in which $\hat{\lambda}_1$ is the largest eigenvalue of $S$. Here $\mathbf{x}_j' = [x_{j1}, x_{j2}, \dots, x_{jp}]$ is the jth observation on p variables $X_1, X_2, \ldots, X_p$. The linear combination $\hat{\mathbf{e}}_p \mathbf{x}_i$ corresponding to the smallest eigenvalue is also frequently singled out for inspection.

Evaluating Bivariate Normality

• By Result 3.7, the set of bivariate outcomes x such that (x - μ)'Σ^{-1} (x - μ) ≤ χ2^2(0.5) has probability 0.5.

• Thus we should expect roughly the same percentage, 50%, of sample observations lie in the ellipse given by {all x such that (x - ĥ)'S^{-1}(x - ĥ) ≤ χ2^2(0.5)} where μ is replaced by ĥ and Σ^{-1} by its estimate S^{-1}. If not, the normality assumption is suspect.

Example 3.12 (Checking bivariate normality)

Although not a random sample, data consisting of the pairs of observations (x1 = sales, x2 = profits) for the 10 largest companies in the world are listed in the following table. Check if (x1, x2) follows bivariate normal distribution.

39

The World's 10 Largest Companies¹

Company         Profits (billions)    Sales (billions)     Assets (billions)
Citigroup      108.28               17.05              1,484.10
General Electric   152.36             16.59              750.33
American Intl Group 95.04              10.91              1,110.46
Bank of America    65.45              14.14              766.42
HSBC Group       62.97               9.52              1,031.29
ExxonMobil      263.99              25.33              195.26
Royal Dutch/Shell   265.19             18.54              193.83
BP                285.06             191.11             15.73
ING Group        92.01               8.10              1,175.16
Toyota Motor    165.68              211.15             11.13

¹ From www.Forbes.com, partially based on Forbes Global 2000, April 18, 2005.

A somewhat more formal method for judging normality of a dataset is based

On the squared generalized distances <math>d_i^2 = (\mathbf{x}_i - \bar{\mathbf{x}})' \mathbf{S}^{-1} (\mathbf{x}_i - \bar{\mathbf{x}})</math> When the parent population is multivariate normal and both <math>n</math> and <math>n-p</math> are greater than 25 or 30, each of the squared distances <math>d_1^2, d_2^2, ..., d_n^2</math> should behave like a chi-square random variable. • Although these distances are not independent or exactly chi-square distributed, it is helpful to plot them as if they were. The resulting plot is called a <i>chi-square plot</i> or <i>gamma plot</i>, because the chi-square distribution is a special case of the more general gamma distribution. To construct the chi-square plot, order the square distances in the equation above from smallest to largest. Largest as d(1)² ≤ d(2)² ≤ … ≤ d(n)². Graph the pairs (qc,p)((j-1/2))/n, d(j)², where qc,p)((j-1/2))/n is the 100(j-1/2)(1/2)/n quantile of the chi-square distribution with p degrees of freedom.

Example 3.13 (Constructing a chi-square plot) Let us construct a chi-square plot of the generalized distances given in Example 3.12. The order distance and the corresponding chi-square percentile for p=2 and n=10 are listed in the following table:
q_c,2((j-1/2)/10)
d_(j)^2
j
0.10
0.30
1
0.33
2 3
0.62
0.58
1.16
4
0.86
1.30
5
1.20
1.61
6
1.60
1.64
7
2.10
1.71
8
1.79
2.77
9
3.53
3.79
5.99
10
4.38
42

A chi-square plot of the ordered distances in Example 4.13.

Figure 4.7 shows a chi-square plot of the ordered distances in Example 4.13. The plot displays the squared distances d_{ij}^{2} and d_{(j)}^{2}, as well as the quantile function q_{c,2}((j-1/2)/10) and q_{c,4}((j-1/2)/30). The x-axis represents the ordered distance, while the y-axis shows the corresponding chi-square value.

The two plots in Figure 4.8 are chi-square plots for two simulated four-variate normal data sets with n = 30.

Example 3.14: Evaluating multivariate normality for a four-variable data set

The data in Table 4.3 were obtained by taking four different measures of stiffness, x1, x2, x3, and x4, of each of n = 30 boards. The first measurement involving sending a shock wave down the board, the second measurement is determined while vibrating the board, and the last two measurements are obtained from static tests.

The squared distances d_j = (x_i - bar{x})' S^(-1) (x_i - bar{x}) are also presented in the table. Table 4.3: Four Measurements of Stiffness

Observation    Observation    d^2
no.   d^2   x1   no.   x4   x1   x2   x3 The data includes variables x2, x3, and chi4, along with other numerical values. The numbers are as follows:

2149
1180
1889
1651
1561
1778
16
1954 0.60
1281 1 16.85
1325
1170
2087
2197 5.48
1002
2403
2048
1176 3.50
1371
1700
1815
2222
18
1419
1252
1308
2119 7.62 3.99
1828
1634
1602 5.21
1755
1627
1110
1533
1645 1.36
1725
1883 1.40
1594
1313
1646
1916
1614
1976 1.46
2276
1439 2.22
2189
1547
2111 6
1712
1712
1546 9.90
1899
1614
1422
1477
1943
1685
1271
1671 4.99 5.06
1290
1874
1633
1513
1516
2104
1820
1717 1.49 0.80
2061
1867
1646
2037 2.54
2983
2794
2412
2581 12.26
1493
1356
1533 10
1600
1384
1508 0.77
1856 4.58
1745
1412
1238
1469 3.40
1591
1518
1667 1.93
1727
1710
2.38
1896
1701
1834 12
1907
1627
1898 0.46
2168
2046
3.00
1675
1597
1841
1595 2.70
1655
1414 13
1840
1741 6.28
2234
1493
1678
2326 2301, 2065, 14, 1685, .13, 1867, 2.58, 1214, 1284, 1389, 30, 1490, 1382, 15, 1859, 1649, 1714, 1.08
Source: Data courtesy of William Galligan.
44

d(j)² ⊙ 16 4 - ⊙ 12 01 • ∞ -

A chi-square plot for the data in Example 4.14.

The data appears to be:

9
4
0
0
12
10
2
4
6
8
0

And there is a mathematical equation:

q_{c,4}((j-\frac{1}{2})/30)

Note: The "Figure 4.9" heading was not included in the original text, so I did not add it back in.

3.7 Detecting Outliers and Cleaning Data

• Outliers are best detected visually whenever possible.
• For a single random variable, the problem is one-dimensional, and we look for observations that are far from the others.
• In the bivariate case, the situation is more complicated. Figure 4.10 shows a situation with two unusual observations.
• In higher dimensions, there can be outliers that cannot be detected from univariate plots or even bivariate scatter plots. Here, a large value of (∗x_i - x̄)S^(-1)(∗x_i - x̄) will suggest an unusual observation, even though it cannot be seen visually.

x2

Two outliers: one univariate and one bivariate.

Note: I did not touch the math equation, as per instruction #3. The original OCR text was not provided, so I couldn't apply instructions #1-2 to fix typos or merge broken lines.

Steps for Detecting Outliers
1. Make a dot plot for each variable.
2. Make a scatter plot for each pair of variables.
3. Calculate the standardized variable zjk = (xjk - x̄k)/√skk) for j = 1, 2, ..., n and each column k=1,2,...,p. Examine these standardized values for large or small values.
4. Calculate the generalized squared distance (x_i - x̄)'S^(-1)(x_i - x̄). Examine these distances for unusually values. In a chi-square plot, these would be the points farthest from the origin.

Note: The original text contained mathematical equations and symbols that were left intact to preserve their meaning. .06 -0.15 1602 .05 .28 -0.12 1770 1677 1707 1785 .43 1738 .64 1.07 .94 .60 1376 1190 723 1285 2791 -2.87 -0.73 (4.57) -1.01 -1.47 1705 1577 1332 -0.43 1703 1664 .37 .13 .04 .81 1643 1535 -0.12 .28 1510 1582 .11 1494 .04 -0.20 1567 1510 -0.56 1301 -0.22 -0.28 1405 1553 -0.21 -0.31 1528 1591 1714 -0.38 .10 .75 1685 1698 1.10 .26 1803 1826 1748 .78 1.23 2746 1764 1.01 (4.65) .52 1587 1554 1352 -0.05 -0.35 .26 1554 1551 -0.13 -0.32 :: 48

Example 3.15 (Detecting outliers in the data on lumber)

Table 4.4 contains the data in Table 4.3, along with the standardized observations. These data consist of four different measurements of stiffness x1, x2, x3, and x4, on each n=30 boards. Detect outliers in these data.

Table 4.4: Four Measurements of Stiffness with Standardized Values

d^2 Observation no. x4 x1 x2 x3 z1 z2 z3 z4
1778 1889 1651 1561 -0.1 -0.3 0.2 1 0.2 0.6
2 1.5
2048 2197 2403 2087 1.9 1.5 5.48
3 2119 1700 1815 2222 0.7 -0.2 1.0 1.5 7.62
-0.8
4 1645 1627 1533 -0.4 -1.3 5.21
1110 1916 5.2 0.5 1.40 1976 1614 1883 0.5 0.3
2.22 1712 1712 1439 1546 6
-.6 -.1 -.2 -.6 -.2 1943 1685
-.8 4.99 1271 1671 0.1 -.2 8 2104 1820 1717 0.2 0.5 1.49 1874 0.6 0.7 12.26 9 3.3 2983 2794 3.3 3.0 2.7 2412 2581 0.77 -.5 1384 1508 -.5 -.7 1745 1600
-.4 1.93 -.5 -.2 1710 1591 -.6 0. 1518 1667
11 ,46 2046 1907 1627 1898
12 .4 .5 .5 .4 2.70 -.2 .3 1840 1841 1595
13 .3 0. 1741 .13 -.2 1867 1493 1678 -.1 -.1 -.1 1685
14 1.08 -.3 -.4 -.0 1859 1649 1389 1714
15 -.1 16.85 0.1 1.3 -1.1 1954 2149 1180 1281 -1.4 16 3.50 -1.8 -1.8 -1.7 1325 1170 1002 1176 17 -1.7 3.99 1419 -1.5 -1.2 -1.3 1371 1252 1308 18 -.8 1.36 1634 1828 1602 1755 19 -.2 -.4 .3 .1 1.46 1725 -.6 -.5 -.2 1594 1313 1646 20 -.6 9.90 2276 2189 1547 21 1.1 .1 1.2 2111 1.4 5.06 1899 22 -.0 -.3 -.8 1614 1422 1477 -.4 ,80 -.8 -.7 1633 1513 23 -.7 -.6 1290 1516 2.54 4.58 1867 .5 .5 2061 1646 2037 24 .4 1.0 25 1856 1493 1533 -.2 -.8 -.5 -.6 1356 3.40
2.38
3.00
6.28 -.9 -.8 1727 1412 1238 1469 26 -.6 -1.1 .8 .3 2168 1896 27 .5 .6 1701 1834 -.8 -.2 -.3 1655 1675 1414 1597 28 -.4 1.3 1.7 1.6 2326 2301 2065 2234 29 1.8 2.58 -1.2 1490 1382 1214 1284 30 -1.3 -1.0 -1.4 49

Figure 4.11: Scatter plots for the lumber stiffness data with specimens 9 and 16 plotted as solid dots.

1500
2500
1800
1200
2400

16
2500
0
8
00
0
0
200
0
x1
• 9
8
1500
8
0
000
8
0
0
0
•

2500
0
0
00
0
0
0
998
x2
∩
0 000
80
08
80
0
1500
000
Ø 00
0
0
0
•

2200
0
8
9
0
0
0
1600
00
х3
0
0
0
0
0
0
80
8
8
0
0
0
1000
0
0
•

2400
0
0
0
8
0
0
0
~ 600
600
600
0
0
0
00
1800
000
x4
( ) ( ) ( ) ( ) ( ) ( ) ( ) ( ) ( ) ( )
1200
8
•
0
0
0
1500
2500
1000
1600
2200

3.8 Transformations to Near Normality

If normality is not a viable assumption, what is the next step?

• Ignore the findings of a normality check and proceed as if the data were normally distributed (not recommended).

To make nonnormal data more "normal looking," consider making transformations of the data. Normal-theory analyses can then be carried out with the suitably transformed data.

Appropriate transformations are suggested by:

1. theoretical consideration
2. the data themselves.

Helpful Transformations to Near Normality

Original Scale Transformed Scale
1. Counts, y √y
2. Proportions, p logit = 1/2 log((p)/(1-p))
3. Correlations, r Fisher's z(r) = 1/2 log((1+r)/(1-r))

Box and Cox Transformation

Please let me know if you need any further assistance!

Given the observations x1, x2, ..., xn, the Box-Cox transformation for the choice of an appropriate power λ is the solution that maximizes the expression:

x^(λ) = { (x^λ - 1)/λ if λ ≠ 0
ln x if λ = 0

or

yj^(λ) = (xj^λ - 1)/(λ [(∏i=1nx_i)^1/n]^(λ-1)), j = 1, ..., n

The Box-Cox transformation for the choice of an appropriate power λ is the solution that maximizes the expression:

ℓ(λ) = -(n/2) ln |(1/n) ∑j=1n (xj^(λ) - xj^(λ))^2| + (λ-1) ∑j=1n ln xj

where x^(¯(λ)) = (1/n) ∑j=1n ((xj^λ - 1)/λ).

Example 3.16 (Determining a power transformation for univariate data)

We gave readings of microwave radiation emitted through the closed doors of n=42 ovens in Example 3.10. The Q-Q plot of these data in Figure 4.6 indicates that the observations deviate from what would be expected if they were normally distributed. Since all the positive observations are positive, let us perform a power transformation of the data which, we hope, will produce results that are more nearly normal. We must find that value of λ that maximizes the function ℓ(λ).

ℓ(λ) = -1.00 × 0.70^λ + (-0.90) × 0.75^λ + 0.40 × 0.80^λ + (-0.80) × 0.84^λ + (-0.70) × 0.85^λ + 0.50 × 0.86^λ + (-0.60) × 0.87^λ + (-0.50) × 0.88^λ + (-0.40) × 0.90^λ

The values of ℓ(λ) for λ = -1.00, -0.90, 0.40, -0.80, 0.50, -0.70, 0.60, and -0.50 are:

70.52
-1.00
-.90
75.65
.40
106.20
-.80
80.46
.50
105.50
-.70
84.94
.60
104.43 -.60 89.06 .70 103.03 -.50 92.79 .80 101.33 -.40 96.10 .90 99.34 -.30 98.97 97.10 1.00 -.20 1.10 101.39 94.64 -.10 103.35 1.20 91.96 104.83 .00 1.30 89.10 .10 105.84 1.40 86.07 .20 106.39 1.50 82.88 106.51 (.30) 53

Figure 4.13 A Q–Q plot of the transformed radiation data (door closed). The integers in the plot indicate the number of points occupying the same location.

X^(1/4)
-0.50
-1.00
-1.50
-2.00
-2.50
-3.00
=q_(j)

2.0
1.0
3.0
-2.0
-1.0
0.0

Figure 4.12 Plot of ℓ(λ) versus λ for radiation data (door closed).

ℓ(λ)
106.5
0.901
105.5
105.0
→ λ
0.5
0.0
0.1
0.2
0.3
0.4
λ = 0.28
54

Transforming Multivariate Observations

With multivariate observations, a power transformation must be selected for

Each of the variables. Let λ1, λ2, …, λp be the power transformations for the p measured characteristics. Each λk can be selected by maximizing ℓ(λ) = -n/2 ln | 1/n ∑i=1n (xjk^(λk) - x¯k^(λk))^2 | + (λk - 1) ∑i=1n ln xjk where x1k, x2k, …, xnk are n observations on the kth variable, k = 1, 2, ..., p. Here x¯k^(λk) = 1/n ∑k=1n (xjk^λk - 1)/λk • Let λ1, λ2, …, λp Be the values that individually maximize the equation above. Then the jth transformed multivariate observation is $\mathbf{x}_j^{(\hat{\boldsymbol{\lambda}})} = \left|\frac{x_{j1}^{\lambda_1}-1}{\hat{\lambda}_1}, \frac{x_{j2}^{\lambda_2}-1}{\hat{\lambda}_2}, \cdots, \frac{x_{jp}^{\lambda_p}-1}{\hat{\lambda}_n}\right|$

The procedure just described is equivalent to making each marginal distribution approximately normal. Although normal marginals are not sufficient to ensure that the joint distribution is normal, in practical applications this may be good enough.

If not, the value $\hat{\lambda}_1, \hat{\lambda}_2, \dots, \hat{\lambda}_p$ can be obtained from the preceding transformations and iterate toward the set of values $\lambda' = [\lambda_1, \lambda_2, \dots, \lambda_p]$, which collectively maximizes $\ell(\lambda_1, \lambda_2, \dots, \lambda_p) = -\frac{n}{2} \ln |\mathbf{S}(\lambda)| + (\lambda_1 - 1) \sum_{j=1}^{n} \ln x_{j1} + (\lambda_2 - 1) \sum_{j=1}^{n} \ln x_{j2}$ $+ \cdots + (\lambda_p - 1) \sum_{n=1}^{n} \ln x_{jp}$ <S> is the sample covariance matrix computed from <x>j^(λ) = [ (xj1^λ1 - 1)/λ1, (xj2^λ2 - 1)/λ2, ..., (xjp^λp - 1)/λp ], j = 1, 2, ..., n

Example 3.17 (Determining power transformations for bivariate data)

Radiation measurements were also recorded through the open doors of the n=42 microwave ovens introduced in Example 3.10. The amount of radiation emitted through the open doors of these ovens is listed in Table 4.5. Denote the door-close data x11, x21, ..., x421 and the door-open data x12, x22, ..., x422. Consider the joint distribution of x1 and x2, choosing a power transformation for (x1, x2) to make the joint distribution of (x1, x2) approximately bivariate normal.

Table 4.5: Radiation Data (Door Open)

Oven no. 1 2 3
Radiation .20 .10 .09 17, 3, 33, 0.30, 18, 0.10, 0.10, 4, 34, 0.01, 0.30, 0.10
19, 5, 35, 20, 0.60, 0.12, 0.10, 36, 0.25, 6, 0.12
21, 0.12, 7, 22, 37, 0.20, 0.09, 0.10, 8, 23, 38, 0.05, 0.40, 0.10, 39, 0.33, 24, 0.05
9, 0.09, 25, 0.32, 10, 0.15, 40, 0.10, 0.12, 26, 0.30, 41, 0.07
11, 42, 0.12, 27, 0.15, 12, 0.05, 13, 28, 0.09, 0.01, 29, 0.45, 0.09
14, 30, 0.28, 0.12, 15, 57

Source: Data courtesy of J. D. Cryer.

(a)
x{(j)}^(1/4)

.00
-.60
-1.20
-1.80
-2.40
-3.00
→ q(j)
-2.0
-1.0
0.
1.0
2.0
3.0

(b)
Figure 4.14 Q–Q plots of (a) the original and (b) the transformed radiation data (with door open). (The integers in the plot indicate the number of points occupying the same location.)
58

Figure 4.15: Contour plot of λ2 for the radiation data.

0.5 -
221
221
222
220
222
0.4 -
0.3 -
223
225.5
0.2 -
225.9
225
0.1
0.0 -
223
224
222
221

- \ \
0.3
0.2
0.0
0.1
0.5
0.4

If the data includes some large negative values and has a single long tail, a more general transformation should be applied.

x^(λ) = ⎧⎨⎩{(x+1)^λ - 1}/λ if x ≥ 0, λ ≠ 0
ln(x+1) if x ≥ 0, λ = 0
-( (-x+1)^(2-λ) - 1)/(2-λ) if x < 0, λ ≠ 2
-ln(-x+1) if x < 0, λ = 2⎫⎬⎭