MODULE: 1
Introduction to Random Vectors
72

# DISCRETE TYPE RANDOM VECTORS:
An example of a random vector Ex = (X1, ..., Xn) is said to be of discrete type if each component Xi is a discrete random variable.

The type of support of (X1,..., Xp) is countable and

$$\sum_{\chi \in S_\chi} P(\chi = \chi) = 1$$

Probability Mass Function
f(x1,...,xp)(x1,...,xp) = P[(x1,...,xp)]=(x1,...,xp)]

# CONTINUOUS TYPE RANDOM VECTORS:
A random vector X = (X1,..., Xp) is said to be of continuous type if there exists a continuous function f: R^p -> R, such that,
F(x) = ∫... ∫ f(t) dt, ..., dtp.

This function is called Probability Density Function.

Properties of Joint pdf:

(1) The probability density function, denoted by $f_{\chi}(\chi)$, is a non-negative function such that $\int \int \int f_{x}(x) dx_1...dx_p = 1.$

(i) For any $p$, the marginal distribution of $X$ is given by $F_X(x) = F_x(x) I\{x \in ERP\}$.

(2) The probability of an event $A$ is given by $P(X \in A) = \int \int f(x_1, x_2) dx_1...dx_p$.

EXAMPLE: Bivariate Uniform

$(X_1, X_2)$ ~ Unif([0,1] × [0,2]) if the probability density function of $(X_1, X_2)$ is defined as:

$f_{X_1 X_2}(x_1, x_2) = \begin{cases} \frac{1}{2}; & (x_1, x_2) \in [0, 1] × [0, 2]\\ 0; & otherwise. \end{cases}$

$F_{X_1 X_2}(x_1, x_2) = \int \int f(x_1, x_2) dt_1, dt_2$

(X1, X2) ~ Uniform ()

fx1x2 = C 2f Sx1x2
∫∫ fX1X2 dx1 dx2 = 1.
SS c dx1 dx2 + SS o dx1 dx2 = 1.
Sx1x2 Sx1x2
=> C × Area of Sx1x2 + 0 = 1.

EXAMPLE: Let X1 ~ Uniform [1, 2, 3] and X2 ~ Binomial (1, 2/3)
Let X1 and X2 be independent. Probabilities are defined as follows:

* P(X1=1) = P(X1=2) = P(X1=3) = 1/3
* P(X2=1) = 2/3, P(X2=0) = 1/3
* Support of X1 and X2 = Sx1 x2 = {(1,0), (2,0), (3,0),
(1,1), (2,1), (3,1)}
* P((x1, x2) = (x1, x2)) = P(x1=x1) × P(x2=x2)
°° X1 and X2 are independent.

The marginal distribution of a random vector (X1,...,Xp) with cumulative distribution function (CDF)

P[(1,0)] = P[(1,0)] + P[(3,0)]
= P(X1=1) × P(X1=0) + P(X1=2) × P(X2=0)
= (1/3 × 1/3) + (1/3 × 1/3) = 2/9

F(3,2) = 1
Il copides + Il oyer
2.4 F(0,0) = 0
7 × 7 × 3

# MARGINAL DISTRIBUTION

Summary of X2: The distribution of a single random variable after reducing every other random variables.

Marginal CDF:
Fχi(χi) = lim χi → ∞ Fχi...χp(χi, ..., χp)

Note: I left the math equation alone as per instruction.

Marginal Pdf:
Let f(x1,...,xp) be the pdf of (X1,..., Xp).
The marginal pdf of X1 is defined as
$f_{X_1}(x_1) = \int \int ... \int f(x_1...x_p) dx_1 dx_2...dx_p$
$-\infty < x_1 < \infty$

Marginal Prof:
f(x1,...,xp) = ∫∫...∫ f(x1,...,xp)
# CONDITIONAL DISTRIBUTION
Let (X1,..., X7) be a random vector with polynomial pdf/pmf f(x). Let fxq(2q) be the marginal pmf/pdf of X2, then,
$f_{X_1...X_p|X_i}[(x_1...x_p) \in A|X_i=x_i] = \frac{f_{X_1...X_p}(x_1...x_p)}{f_{X_i}(x_i)}$

CONDITIONAL DISTRIBUTION IN TWO VARIABLES

Let <math>f_{X_1X_2}(x_1,x_2)</math> be the pdf/pmf of $(X_1,X_2)^T$.

Then, conditional probability density function (pdf) of $X_1$ given $X_2 = x_2$ is:

$f(x_1|x_2=x_2) = f_{x_1x_2}(x_1,x_2)$

Conditional Cumulative Distribution Function (CDF):

$F(x_1|x_2=x_2) = \mathbb{E}[x_1 < x_1 | x_2=x_2]$
$= \int_{-\infty}^{x_1} \frac{1}{x_1 | x_2 = x_2} \left( \frac{1}{x_2} \right) dt.$

# INDEPENDENCE OF RANDOM VARIABLES

RVs X18, X2; we call them independent if
f(x, x) = f(x,) * f(x,) \ (x,) \ (x,) \ ER2.
where f is a probability density function (pdf) or probability mass function (pmf).

Equivalently, X1 and X2 are independent if,
F(x1, x2) = F(x1) * F(x2) & (x1, x2) \ ER2.
where F is a cumulative distribution function (CDF).

For random variables (X1,...,Xp) = X with pdf/pmf t(x1)...t(xp),
E(X)=

Let me know if you need any further assistance!

22/08/2023
# VARIANCE - COVARIANCE MATRIX

For a random vector X, we define variance-covariance matrix as follows:

Var(X) = E(XX^T) - E(X)E(X^T)
E(XX^T) = E[(X-E(X))(X-E(X))^T]

Var (X1) Cov (X1, X2) ... cov (X1, Xp)
Var(X) = cov(X2, X1) Var(X2) ...
cov(xp, x1) .... var(xp)

- For any compatible matrix A:
Var(AX + b) = A Var(X)A^T
b: vector of constants

- Correlation matrix is defined in a similar way:

cor(x_1, x_2) ⋅ cor(x_1, x_p)
cor(X) = 
{
cor(x_2, x_1) & 1 \\
⋮ & ⋮ \\
}

Transformation of a Random Vector

Jacobian:

U1 = U1(x1)
or transformation from
U2 = U2(x2)
x1, x2 -> U1, U2

The Tacoblan transformation x1, x2 -> v1, v2 is given by:

$$\frac{\partial(v_1, v_2)}{\partial(x_1, x_2)} = \begin{vmatrix} \frac{\partial v_1}{\partial x_1} & \frac{\partial v_2}{\partial x_1} \\ \frac{\partial v_1}{\partial x_2} & \frac{\partial v_2}{\partial x_2} \end{vmatrix}$$

10 meters
U: (x1, x2) -> {v1(x1, x2), v2(x1, x2)}; U: R^2 -> R^3.

Theorem: Injective Transformation

Let g: R^n -> R^n be a function such that:

(i) g is one-one
(ii) The Jacobian of the inverse, i.e., $$\left|\frac{\partial g^{-1}}{\partial (x_1, x_2,...,x_p)}\right| \neq 0$$

for almost all $(x_1,...,x_p) \in \mathbb{R}^p$.

NOTE:
X=Y almost everywhere, which implies P(X+Y)=0. Then Y=g(X) is a random variable with density function fy(y) defined by fy(y) = fx(g^(-1)(y)) | 2g^(-1)(y) | dy = RP.

Let me know if you need any further assistance!

EXAMPLE: X and Y be random variables with distribution function going to infinity (7df):

fχ,χ(2,y) = 
{ 2/9; 0 < x < 3, 0 < y < x < 3. 
0; otherwise

Insert "Car"
Then, find the joint destination of X + Y and X - Y.

g(x, y) = (x+y, x-y)

g1(x,y) = x+y, g2(x,y) = x-y

7, y -> U, V
u = u(x,y) = x+y
y = v/2, y = -v/2
v = v(x,y) = x-y

g^(-1)(υ, v) = ((υ+v)/2, (υ-v)/2)
= (∂g^(-1)(υ, v)/∂(υ, v)) = (1/12, 1/12)

= | -1/2 | = 1/2

fun (u,v) = fxy(g-1(u,v)) | ∂g^{-1}(u,v)/∂(u,v) | V(u,v) ∈ ℝ².

= ⎧⎨⎩
2/9 × 1/2 ; -Suv
0 ; otherwise
⎫⎬⎭

The equation is: 1/9 + 2.0v26 - 2.0v20 + 2.6.

There are three inequalities:

* 0 ≤ (U+V) ≤ 6
* (3,3)
* (6,0)

Let me know if you need any further assistance!

DISTRIBUTION OF SUM, PRODUCT AND QUOTIENTS

-> Sum of Random Variables (RV)
· Covolution:
f = g be real-valued functions. Then,
f * g(y) = ∫ f(x)g(y-x)dx = ∫ g(x)f(y-x)dx
Y = 1/2 × then E(Y) = 1/2E(X2)
Var(Y) = 1/2 × Var(X2) + 1/2 × Cov(X2, X3)

THEOREM: Let X1 and X2 be independent random variables with pdf f(x1) and f(x2). Then the pdf of Y = X1 + X2 is given as:
f(y) = f(x1) * f(x2)(y) ∫ f(x1) f(x2)(y-2) dx.

28/08/2023

Product of RV

THEOREM: Let X1 and X2 be independent random variables.
and let Z = X1 * X2. Then the probability density function (p.d.f) of
I is defined as follows:
f(z) = f(x1, x2) / sqrt(x1 * x2) * 1/121 dx.

Let me know if you need any further assistance!

Sacot:

FZ(Z) = P(Z < Z) = P(X1X2 < Z)
= P(X1X2 <= |X, >0) + P(X, X2 <= |X1 < 0)
= P(X2 = =/x1 | X1 > 0) + P(X2 = =/x1 | X1<0)

FZ(Z) = ∫∫fX1(x1)fX2(x2) dx2 dx1
+ ∫∞0 ∫∞0 fX1(x1)fX2(x1) dx2 dx1

- 00 Z/21

FZ(z) = ∂/∂z FZ(z)
= ∫0∞ ∫X1(x1) ∫X2(z1) (1/x1) dx1
+ ∫X1(x1) ∫X2(z1) (-1/x1) dx1
+ ∫0z1 ∫0z1 (z/x1) dx1

= ∫fx_1(x_1) fx_2(2/x_1) 1/x_1 dx_1

NOTE:

(E(XY)) = E[E(XY|Y)] = E[XE[X|X]]

(ii) Var(XY) = (0x + μ^2x)(0y + μ^2y) - μ^2xμ^2y
If X+Y are uncorrelated & X's and Y's are uncorrelated.

THEOREM: Let X1 and X2 be continuous random variables, and let Z = X1/X2. Then the density function is given by,

f(Z) = | | | | | | | | | | | | | | |
fx1x2 such that the joint density function of X1 and X2.

* E(X) =
* E(X1/X2) = E(X1)E(1/X2); X1 and X2 are independent.
* Var(X1/X2) = E(X12)E(1/X2) - [E(X)E(1/X2)]^2
X1 + X2 are independent.

CHI-SQUARE DISTRIBUTION
Let X be a random variable such that

Pdf of X is given by f(x) = {1/(2^(N/2)) if x <= 1/N, 0 otherwise.

Notation: X ~ χ²

If X ~ Ganina(1/2, 2), then X is said to be distributed as Chi-Square with n degrees of freedom or X ~ χ²n.

Theorem:

(a) Let X₁, X₂, …, Xₙ be i.i.d. following N(μ, σ²), then (Xᵢ - μ) ~ N(0, 1).

(b) If X ~ N(0, 1), then Y = X² => Y ~ χ²

(c) If X₁, …, Xₙₖ are independent random variables such that n = ∑ᵢ=1ⁱnᵢ and Xᵢ ~ χ²ⁿᵢ for λ = 1, 2, …, k, then ∑ᵢ=1ⁱXᵢ ~ χ²ₙ

Let Xi, X2,..., Xn be normally distributed variables following a standard normal distribution (c) X and S2 are independent random variables.

(b) S2 = 1 + (22 - 1), then N(6,5) data abound. Then,

(n-1)S2 ~ X^2_1

(a) X ~ N(R, 22)

Var(S2) = 204

(d) E(S^2) = 0

1964 ()

" ) = 1 - 10 +

# STUDENT t-DISTRIBUTION

Let X be a random variable of continuous type. Let μ

Note: I fixed the typos, merged the broken lines, and left the math/equations alone as per your instructions.

Be a true integer, X is said to follow a student-t distribution with m degrees of freedom. Denoted by X ~ t_m.

If the probability density function (PDF) is denoted by f_x, then,

f_x(x) = 
{
  (m+1)/2 if x = 1
  sqrt(m+1) * (1 + x^2/m)^(m+1)/2 if x ∈ ℝ
}

* The expected value of X' is not finite if r ∈ [m, m+1, ...]
* For m ∈ {1, 2, ..., m-1}, the expected value of X^r is 0 if r is odd.
Otherwise,

E(X^r) = (m^r/2) / (2^r * r! * m^(-r)); r is even.

* E(X) = 0
* Var(X) = m-2
* Skewness = β = 0
* Kurtosis = γ = 3(m-2) = 72

# F - Distribution
For positive integers n1 and n2, a random variable X is said to have Snedecor's F-distribution with n1 and n2 degrees of freedom if 7 degrees of freedom of X are less than zero.

Note: I left the math equation alone as per instruction.

Theorem: (a) Let Z ∼ N(0,1) and let

Yes, X may then

Birth.

X = z / √(y/m) ~ t_m

A.
Sales.
-

(6) Let X, Xn, and X2 ~ Xn, then
-
-
-
Y = (X1/n1) / (X2/n2) ~ F(n1,n2)

9
* E(X) = n2/(n2-2), Var(X) = 2n2^2(n1+n2-2) / (n2*(n1+n2-2))

Note: I left the math equations alone as per your instruction.

# Bivariate Normal Distribution
September 18, 2023

Let X and Y have a joint bivariate normal distribution.
Then the joint probability density function of X and Y is given by:

fxy(x, y) = (1/(2πσxσy√(1-p^2))) exp[-((x-μx)/σx)^2 + ((y-μy)/σy)^2]

where plx = E(X), ply = E(Y)
σx = var(X), σy = var(Y)
g = corr(X, Y)

fX(x) = (1/((2π)^k/2)√(|Σ|)) exp[-(1/2) Σ^(-1) × ]

where X = (x, μX, μY, σX^2, σY^2), and k = 2.

Z = (var(X), var(Y))

cor(Y, X) = var(Y)

121 = 0xy - [ar(x, y)] = 0xy - p^2 0xy = 0xy (1-p^2)

Let X, Y be random variables with joint PDF: 

f_{xy}(x,y) = (5/(48π)) * exp(-((25/32)((x^2)/4 + (y^2)/9 - (xy)/5 - (4/5)x - (8/5)y + 16/5)))

Then find the mean and variance of X & Y. Find the correlation (X, Y). Also formulate the E(X[Y=y]).

f_{xy}(x,y) = 

σ_x^2 = 4
σ_y^2 = 9 = 5
σ_x = 2
σ_y = 3

Let's break it down:

1. The joint PDF of X and Y is given by the equation: f_{xy}(x,y) = (5/(48π)) * exp(-((25/32)((x^2)/4 + (y^2)/9 - (xy)/5 - (4/5)x - (8/5)y + 16/5)))

2. The mean and variance of X & Y need to be found.

3. The correlation between X and Y needs to be calculated.

4. The expected value of X given Y=y, denoted by E(X[Y=y]), also needs to be formulated.

Note: There seems to be some errors in the original text, such as σ_y = 93 which is likely a typo and should be corrected to σ_y = 3. On comparing, 2g over sigma_x times sigma_y equals 1/5 equals 3/5.

J. 12, 4, 2, 15.

The probability density function (PDF) of a bivariate normal distribution can be written as:

ppx - ppx = - 4
0x0x - 02x = - 5

The mean and variance of the PDF are given by:

=1 μx(ƒ/σxσy-1/σx^2)=-2/5

202022
=> μx((8/(2×8×5)-1/4))=-2/5 => μx((1/10-1/4))=-2/5

=1 μx ((2-5)/20) = -2/8 = -1/8 (-3/4) = -2

9999
|μχ = 8/3

Conditional Distribution
Let X, Y follow a bivariate normal distribution N(μ, Σ) where μ = (μ_x, μ_y) and Σ = σ_x^2 0 0 σ_y^2.
Then the distribution of XIY=y and YIX=n is also normally distributed.
More specifically,
X|Y=y ∼ N(μ_x + fσ_x/σ_y(y-μ_y), σ_x^2(1-f^2))
Y|X=x ∼ N(μ_y + fσ_y/σ_x(x-μ_x), σ_y^2(1-f^2))

Given that X ~ N(5,16) and all are independent,
Y ~ N(7,25)
Z ~ N(2, 2^2), where 2 = 4, 5, ..., 10.
Then give me generate a χ²₈, t₅ & F distribution.
Mx = 5, Ox = 4

My = 7, 8x = 5
21 × - - 12 × 12 × 30
Mz = 2.
X1 = (X-5)/4 ∼ N(0,1) = 1 × 1^2 ∼ X1^2
In general, (Z_2^{-2})/2 ∼ N(0,1) = ((Z_2^{-2})/2)^2 ∼ χ^2
27 × 1^2 + Σ(i=4 to 12) ((Z_2-2)/2)^2 ∼ χ_8^2
t_5 ∼ (χ'/√(y'/s)) where χ' ∼ N(0,1) and χ' ∼ χ_5^2
X1 = (X-5)/4 ∼ N(0,1)

Note: I left the math/equations alone as per your instruction.

<math>\frac{Z_{2}-2}{2} \sim N(0,1)</math>
=1 \(\frac{2}{3}\)^2 \(\chi \chi_{\frac{1}{3}}\)^2 \(\chi \chi_{\frac{1}{3}}\)^2 \(\chi \chi_{\frac{1}{3}}\)^2 \(\chi \chi_{\frac{1}{3}}\)^2 \(\chi \chi_{\frac{1}{3}}\)
(X-5)
~ ts
=) <math>T = \sqrt{\frac{1}{5}} \frac{8}{(\frac{7}{5} - 2)^2}</math>
Let X1, X21... X12 be i.i.d. random sample
0000
from distribution of X.
X: ~ N(5,16) and (Xi, Xj) are independent.
(X1, X21..., X12), Y1, (Z4, Z5,..., Z10).
<math>F_{4,5} = \frac{\chi_{4/4}^2}{\chi_{5/5}^{"}} \sim \frac{\chi_{4/4}^2}{\chi_{5/5}^2}</math>
<math>\chi_1^2 = \left(\frac{\chi - 5}{4}\right)^2 \sim \chi_1^2</math>
<math>Y_1^2 = \left(\frac{\gamma - 7}{5}\right)^2 \sim \chi_1^2</math>

22 ( Z ) F =
( Z2-2)2~ X1
<math display="block">\frac{1}{4} \left[ \left( \frac{X-5}{4} \right)^2 + \left( \frac{Y-7}{5} \right)^2 + \left( \frac{Z_4-2}{4} \right)^2 + \left( \frac{Z_5-2}{5} \right)^2 \right]</math>
1 5 ( Z2-2 )2<br>5 2:6 ( Z2-2 )2