Multiple Linear<br>Regression
<b>UNIT-IV</b>
Dr. Tina Dutta

INTRODUCTION TO MULTIPLE LINEAR
REGRESSION
Multiple linear regression (MLR) is a supervised learning algorithm for finding the
existence of an association relationship between a dependent variable (aka response variable or outcome variable) and several independent variables (aka explanatory variables or predictor variable or features).
Key performance indicators (KPIs) of organizations may be simultaneously
associated with many factors.
For example, revenue generated from a product sold by a company may be simultaneously associated with factors such as price, market size, promotions,
competitor's price, and so on.
In such cases, we try to establish the existence of a relationship between a dependent variable and
•
several independent variables.

The functional relationship
The functional form of MLR is given by
<math>Y_{i} = \beta_{0} + \beta_{1}X_{1i} + \beta_{2}X_{2i} + \cdots + \beta_{k}X_{ki} + \varepsilon_{i}</math>
The regression coefficients <math>\beta_1, \beta_2, \ldots, \beta_k</math> are called partial regression coefficients
The relationship between an explanatory variable and the response
(outcome) variable is calculated after removing (or controlling) the
effect all the other explanatory variables (features) in the model.

ORDINARY LEAST SQUARES ESTIMATION FOR
MULTIPLE LINEAR REGRESSION
<math>Y = X\beta + \varepsilon</math>
Matrix X is not a square matrix and to solve for the regression coefficient we have to
make the matrix a square matrix by multiplying it with <math>X^T</math> (transpose of X)
<math>X^TY = X^TX\hat{\beta}</math>
The regression coefficients <math>\beta</math> is given by
<math display="block">\hat{\boldsymbol{\beta}} = (X^T X)^{-1} X^T Y</math>

ORDINARY LEAST SQUARES ESTIMATION FOR
MULTIPLE LINEAR REGRESSION
The estimated values of response variable are
<math display="block">\hat{Y} = X \hat{\beta} = X(X^T X)^{-1} X^T Y</math>
<math>\hat{Y} = HY</math>
<math>H = X(X^TX)^{-1}X^T</math> is called the <b>hat matrix</b>, also known as the <b>influence matrix</b>,
Hat matrix plays a crucial role in identifying the outliers and influential
observations in the sample.

Data on advertisement revenue (R) of programs along with CTRP and P. Exercise: P, CTRP, R, Serial.

The cumulative television rating points (CTRP) of a television program, money spent on promotion (denoted as P), and the advertisement revenue (in Indian rupees denoted as R) generated over one-month period for 38 different television programs is provided.

Develop a multiple linear regression model to predict the advertisement revenue (R) based on the cumulative television rating points (CTRP) and money spent on promotion (P). Understand the relationship between the advertisement revenue (R) generated as response to variable and promotions (P) and CTRP as predictors. The variables used include: 

10,131,75600,29,137,115200
11,133200,1269960,30,129,141
12,133200,1064760,129600,119,97,31,1229028
13,176400,1207488,100800,115,32,133
14,180000,1186284,147600,1406196,102,33,145
15,133200,1231464,34,149,126000,129,16,147600,1296708,1056384,144,35,122,108000
17,122400,1320648,1415316,153,36,120,194400,1102704,18,96,158400
37,128,176400,1338060,165600,1184316,38,172800,1457400,19,104,117

The MLR model is given by:

R (Advertisement Revenue) = β0 + β1 × CTRP + β2 × P

Model summary
Std. Error of the Estimate
Model
R
Adjusted R-Square
R-Square
0.912a
0.832
0.822
57548.382
aPredictors: (Constant), P, CTRP.
Coefficients
Unstandardized Coefficients Standardized Coefficients
Model
Sig.
t
Std. Error
β
Beta
0.451
0.655
41008.840
90958.920
Constant
CTRP
5931.850
576.622
0.732
10.287
0.000
P
3.136
0.303
0.736
10.344
0.000

The regression model after estimation of the parameters is given by:

R = 41008.84 + 5931.85 * CTRP + 3.136P

For every one unit increase in CTRP, the revenue increases by 5931.85 when the variable promotion is kept constant, and for one unit increase in promotion the revenue increases by 3.136 when CTRP is kept constant.

Assumptions of MLR

1. The regression model is linear in regression parameters (∂-values).
2. The residuals follow a normal distribution and the expected value (mean) of the residuals is zero.
In time series data, residuals are assumed to be uncorrelated.
3.
The variance of the residuals is constant for all values of X_i. When the variance of the residuals
is constant for different values of X_i, it is called homoscedasticity. A non-constant variance of
residuals is called heteroscedasticity.
4. There is no high correlation between independent variables in the model (called multi-collinearity).
Multi-collinearity can destabilize the model and can result in an incorrect estimation of the
regression parameters.

Please provide the OCR text, and I'll be happy to help you rewrite it to make it readable by fixing typos, merging broken lines, leaving math/equations alone, and outputting only the cleaned text.

Residual Analysis

Residuals or errors are the difference between the actual value of the outcome variable and the predicted value (Yi - Ŷi).

Residual (error) analysis is important to check whether the assumptions of regression models have been satisfied. It is performed to check the following:

1. The residuals are normally distributed.
2. Variance of residual is constant (homoscedasticity).
3. The functional form of regression is correctly specified.
4. There are no outliers.

Let me know if you need any further assistance!

Check for Normal Distribution of Residuals

The normality of residuals can be checked using the probability-probability plot (P-P plot).
P-P plot compares the cumulative distribution function of two probability distributions against each other.
P-P plot checks whether the distribution of the residuals matches with that of a normal distribution.
In Python, the ProbPlot() method on statsmodel draws the P-P plot.

Normal P-P Plot

The diagonal line is the cumulative distribution of a normal distribution, whereas the dots represent the cumulative distribution of the residuals.

Sample probabilities: 0.0 to 9.0

If the dots are close to the diagonal line, we can conclude that the residuals follow an approximate normal distribution (we need only an approximate normal distribution).

Theoretical probabilities:

Normal P-P plot of regression standardized residuals.

2. Test of Homoscedasticity

An important assumption of the regression model is that the residuals have constant variance (homoscedasticity) across different values of the predicted value (Y). The homoscedasticity can be observed by drawing a residual plot, which is a graphical representation of the residuals against the predicted values.

Plot between standardized residual value and standardized predicted value. 
If there is heteroscedasticity (non-constant variance of residuals), then a funnel-type shape in the residual plot can be expected.

Residual Plot for checking Homoscedasticity

It can be observed in the figure that standardized residuals are approximately 2.5 and 2.0, respectively.

That the residuals are random and have no funnel shape, which means the residuals have constant variance (homoscedasticity).

-1.0
-1.5
-2
2
-1
0
Standardized predicted values

3. Outlier Analysis

Outliers are observations whose values show a large deviation from the mean.

Value. Presence of an outlier can have a significant influence on the values of regression coefficients. Thus, it is important to identify the existence of outliers in the data.

Outlier Analysis

The following distance measures are useful in identifying influential observations:

1. Z-Score
2. Mahalanobis Distance
3. Cook's Distance
4. Leverage Values

3.1 Z-Score

Z-score is the standardized distance of an observation from its mean value. For the predicted value of the dependent variable Y, the Z-score is given by:

Z = (Yi - ¯Y) / σY

where Yi is the predicted value of Y for ith observation,
¯Y is the mean or expected value of Y,
σY is the standard deviation of Y

Any observation with a Z-score of more than 3 may be flagged as an outlier.