Simple Linear<br>Regression
UNIT-1 and UNIT-2
Dr. Tina Dutta

INTRODUCTION TO SIMPLE LINEAR
REGRESSION
Simple linear regression is a statistical technique for finding the existence of
an association relationship between a dependent variable (aka response
variable or outcome variable) and an independent variable (aka explanatory
variable or predictor variable).
Simple linear regression implies that there is only one independent variable in
the model.

The functional relationship
For a data set with <i>n</i> observations <math>(X_i, Y_i)</math>, where <math>i = 1, 2, ..., n</math>, the functional form can be written
as follows:
<math display="block">Y_{i} = \beta_{0} + \beta_{1}X_{i} + \varepsilon_{i}</math>
where
<math>Y_i</math> is the value of <math>i^{th}</math> observation of the dependent variable in the sample
<math>X_i</math> is the value of <math>i^{th}</math> observation of the independent variable in the sample
<math>\varepsilon_i</math> is the random error (also known as residuals)
<math>\beta_0</math> and <math>\beta_1</math> are the regression parameters (or regression coefficients)

Linear in Parameters
• It is important to note that the linearity condition in linear regression is
defined with respect to the regression coefficients (<math>\beta_0</math> and <math>\beta_1</math>) and not with
respect to the explanatory variables in the model.
• For e.g. the following two regression equations are linear regression models
<math display="block">Y_i = \beta_0 + \beta_1 X_i^2 + \varepsilon_i</math>
<math display="block">Y_i = \beta_0 + \beta_1 \ln(X_i) + \varepsilon_i</math>

Non-Linear in Regression Models
The following regression equations are non-linear regression models
<math display="block">Y_i = \beta_0 + \frac{1}{1 + \beta_1} X_i + \varepsilon_i</math>
<math display="block">Y_i = \beta_0 + e^{\beta_1} X_i + \varepsilon_i</math>

Method of Ordinary Least Squares (OLS)
The method of Ordinary Least Squares (OLS) is used to estimate the regression
parameters.
OLS fits regression line through a set of data points such that the sum of the

squared distances between the actual observations in the sample and the regression
line is minimized.
OLS provides the Best Linear Unbiased Estimate (BLUE)
•
<math display="block">E[\beta - \hat{\beta}] = 0</math>
where <math>\beta</math> is the population parameter and <math>\beta</math> is estimated parameter value from the sample

Deriving the regression coefficients using OLS
• In ordinary least squares, the objective is to find the optimal values
of <math>\beta_0</math> and <math>\beta_1</math> that will minimize the Sum of Squares Errors (SSE)
given in following equation:
<math display="block">SSE = \sum_{i}^{n} \varepsilon_{i}^{2} = \sum_{i}^{n} \left( Y_{i} - \beta_{0} - \beta_{1} X_{i} \right)^{2}</math>
<math>i=1</math>

Deriving the regression coefficients using OLS
To find the optimal values of <math>\beta_0</math> and <math>\beta_1</math> that will minimize SSE, we have to
equate the partial derivative of SSE with respect to <math>\beta_0</math> and <math>\beta_1</math> to zero
<math display="block">\frac{\partial SSE}{\partial \beta_0} = \sum_{i=1}^n -2\left(Y_i - \beta_0 - \beta_1 X_i\right) = 2\left(n\beta_0 + \beta_1 \sum_{i=1}^n X_i - \sum_{i=1}^n Y_i\right) = 0</math>
Solving Eq. for <math>\beta_0</math>, the estimated value of <math>\beta_0</math> is given by
<math>\hat{\beta}_0 = \overline{Y} - \hat{\beta}_1 \overline{X}</math>

Deriving the regression coefficients using OLS
Differentiating SSE with respect to <math>\beta_1</math>, we get
<math display="block">\left| \frac{\partial SSE}{\partial \beta_{i}} \right| = \sum_{i=1}^{n} -2X_{i} \left( Y_{i} - \beta_{0} - \beta_{1} X_{i} \right) = -2 \sum_{i=1}^{n} \left( X_{i} Y_{i} - \beta_{0} X_{i} - \beta_{1} X_{i}^{2} \right) = 0</math>
Substituting the value of <math>\beta_0</math>, we get
<math display="block">\frac{\partial SSE}{\partial \beta_{1}} = -2\sum_{i=1}^{n} (X_{i}Y_{i} - X_{i}\overline{Y} + \beta_{1}X_{i}\overline{X} - \beta_{1}X_{i}^{2}) = \sum_{i=1}^{n} (X_{i}Y_{i} - X_{i}\overline{Y}) - \beta_{1}\sum_{i=1}^{n} (X_{i}^{2} - X_{i}\overline{X})</math>
<math>=0</math>

Deriving the regression coefficients using OLS
Thus, the value of <math>\beta_1</math> is given by
<math display="block">= \frac{\sum_{i=1}^{n} (X_{i}Y_{i} - X_{i}\bar{Y})}{\sum_{i=1}^{n} (X_{i}^{2} - X_{i}\bar{X})} = \frac{n\sum X_{i}Y_{i} - \sum X_{i}\sum Y_{i}}{n\sum X_{i}^{2} - (\sum X_{i})^{2}}</math>
<math>i=1</math>

Other expressions of <math>\widehat{\beta_1}</math>
<math display="block">\hat{\beta}_{1} = \frac{\sum_{i=1}^{n} (X_{i} - \bar{X})(Y_{i} - \bar{Y})}{\sum_{i=1}^{n} (X_{i} - \bar{X})^{2}}</math>
<math display="block">\hat{\beta}_1 = r \times \frac{\sigma_Y}{\sigma_X}</math>
<math display="block">\hat{\beta}_1 = \frac{\text{Cov}(X, Y)}{\text{Var}(X)}</math>
<math>i=1</math>

<b>EXAMPLE</b> Healthcare Treatment Cost versus Age Table-1 provides the treatment cost (in rupees) of 30 patients admitted at the 'Die Another Day (DAD)' hospital for cardiac ailments and their age in years Estimate the regression parameters. Age versus treatment cost (in rupees) Solution: <b>Cost of Treatment</b> Cost of Treatment S. No. Age S. No. Age the estimated values of <math>\beta_0</math> and <math>\beta_1</math> are given by 22.0 112346 125966 16 32.0 1 26.0 128045 18.0 121345 2 17 <math>\hat{\beta}_0 = 118857.35</math> and <math>\hat{\beta}_1 = 348.5559</math> 3 26.5 128104 18 45.0 113290 27.7 141785 4 128584 19 61.0 5 30.0 128699 20 26.0 125098 That is, for every one-year increase in age, 6 31.2 130443 21 71.0 152198 7 8.1 120773 22 7.0 130484 the cost of treatment increases by 348.5559 8 9.0 121293 24.0 131346 23 9 10.0 121981 24 18.0 133026 rupees on an average. 12.0 10 122339 25 45.0 134243 11 7.0 39.0 134648 122550 26 12 14.0 123472 135778 27 57.0 13 11.0 124223 28 40.0 137693 17.0 139067 14 125592 29 51.0 15 21.0 54.0 125683 30 145559 Source: Book: Business Analytics: The Science of Data-driven decision making by U Dinesh Kumar (2017)

Assumptions of Simple Linear Regression
The regression model is linear in regression parameters.
The explanatory variable, <i>X</i>, is assumed to be non-stochastic (i.e., <i>X</i> is deterministic).
The conditional expected value of the residuals, <math>E(\varepsilon_i|X_i)</math>, is zero.
3.
In case of time series data, residuals are uncorrelated, that is, <math>Cov(\varepsilon_i, \varepsilon_j) = 0</math> for all <math>i \neq j</math>.
4.
The residuals, <math>\varepsilon</math>, follow a normal distribution.
5.
<b>6.</b> The variance of the residuals, <math>Var(\varepsilon_i|X_i)</math>, is constant for all values of <math>X_i</math>. When the variance of the
residuals is constant for different values of <math>X_i</math>, it is called <b>homoscedasticity</b>. A non-constant vari-
ance of residuals is called <b>heteroscedasticity</b>.

Gauss-Markov Theorem in Regression Analysis
The Gauss-Markov theorem states that:
• For a regression model with the assumptions <math>E(\varepsilon) = 0</math>, <math>Var(\varepsilon) = \sigma^2</math>, and
uncorrelated errors,
• the least-squares estimators are unbiased and have minimum variance when
compared with all other unbiased estimators that are linear combinations of
the yi.
Also, the least-squares estimators are best linear unbiased estimators, where
"best" implies minimum variance.

VALIDATION OF THE SIMPLE LINEAR REGRESSION
MODEL
It is important to validate the regression model to ensure its validity and goodness of fit
before it can be used for practical applications.
Measures used to validate the simple linear regression models:
1. Co-efficient of determination (<i>R</i>-square).
2. Hypothesis test for the regression coefficient <math>\beta_1</math>.
3. Analysis of Variance for overall model validity (relevant more for multiple linear regression).
4. Residual analysis to validate the regression model assumptions.
5. Outlier analysis.

1. Coefficient of Determination (R-Square)
• The <b>coefficient of determination</b> measures the proportion of variation in
Y that is explained by the variation in the independent variable X in the
regression model.
• The range of <math>r^2</math> is from 0 to 1, and the greater the value, the more the
variation in Y in the regression model can be explained by the variation in X.
<math display="block">r^2 = \frac{\text{Regression sum of squares}}{\text{Total sum of squares}} = \frac{SSR}{SST}</math>

2. Hypothesis test for slope (<math>\beta_1</math>) of Regression
• The regression co-efficient <math>(\beta_1)</math> captures the existence of a linear
relationship between the response variable and the explanatory variable.
• If <math>\beta_1 = 0</math>, we can conclude that there is no statistically significant linear
relationship between the two variables.
The null and alternative hypotheses for the SLR model can be stated as follows:
<math>H_0: \beta_1 = 0</math>
<math>H_0</math>: There is no relationship between <math>X</math> and <math>Y</math>
OR
<math>H_{\Delta}</math>: <math>\beta_1 \neq 0</math>
<math>H_{\Delta}</math>: There is a relationship between <math>X</math> and <math>Y</math>

Hypothesis test for slope (<math>\beta_1</math>) of Regression
The corresponding <i>t</i>-statistic is given as follows:
<math display="block">t = \frac{\hat{\beta}_{1} - \beta_{1}}{S_{e}(\hat{\beta}_{1})} = \frac{\hat{\beta}_{1} - 0}{S_{e}(\hat{\beta}_{1})} = \frac{\hat{\beta}_{1}}{S_{e}(\hat{\beta}_{1})}</math>
This is a two-tailed t-test
with (n-2) degrees of
freedom
<math>S_{\alpha}(\hat{\beta}_1)</math> is the standard error of <math>\hat{\beta}_1</math>
<math display="block">S_{e}(\hat{\beta}_{1}) = \frac{\sqrt{\sum_{i=1}^{n} (Y_{i} - \hat{Y}_{i})^{2} / n - 2}}{\sqrt{\sum_{i=1}^{n} (X_{i} - \bar{X})^{2}}}</math>

3. Analysis of Variance for Overall Model
Validity (F-test)
Using the Analysis of Variance (ANOVA), we can test whether the overall
model is statistically significant.
• However, for a simple linear regression, the null and alternative hypotheses
in ANOVA and t-test are exactly same.
• When using the least-squares method to determine the regression
coefficients one needs to compute three measures of variation.

3. Analysis of Variance for Overall Model
Validity (F-test)
• The first measure, the total sum of squares (SST), is a measure of variation
of the Yi values around their mean, Y
• The total variation, or total sum of squares, is subdivided into explained
variation and unexplained variation.
• The explained variation, or regression sum of squares (SSR), represents the
variation that is explained by the relationship between X and Y.
• The unexplained variation, or error sum of squares (SSE), represents
variation due to factors other than the relationship between X and Y.

Analysis of Variance
Contribution to
Y
error sum of squares
Yi
<math display="block">\widehat{Y}_i = b_0 + b_1 X_i</math>
Contribution to total
sum of squares
Contribution to
regression sum of
squares
<math>\overline{v}</math>
X
0
X;

Computation of Sum of Squares (Variances)

COMPUTATIONAL FORMULAS FOR SST, SSR, AND SSE

<math display="block">SST = \sum_{i=1}^{n} (Y_i - \overline{Y})^2 = \sum_{i=1}^{n} Y_i^2 - \frac{\left(\sum_{i=1}^{n} Y_i\right)^2}{n}</math>
<math display="block">SSR = \sum_{i=1}^{n} (\hat{Y}_i - \overline{Y})^2 = b_0 \sum_{i=1}^{n} Y_i + b_1 \sum_{i=1}^{n} X_i Y_i - \frac{\left(\sum_{i=1}^{n} Y_i\right)^2}{n}</math>
<math display="block">SSE = \sum_{i=1}^{n} (Y_i - \hat{Y}_i)^2 = \sum_{i=1}^{n} Y_i^2 - b_0 \sum_{i=1}^{n} Y_i - b_1 \sum_{i=1}^{n} X_i Y_i</math>

F-test for testing significance of the Regression
Model
The null and alternative hypothesis for <i>F</i>-test are given by
<math>H_0</math>: There is no statistically significant relationship between Y and any of the
explanatory variables (i.e., all regression coefficients are zero).
<math>H_{\Delta}</math>: Not all regression coefficients are zero.
Alternatively:
<math>H_0</math>: All regression coefficients are equal to zero.
<math>H_{A}</math>: Not all regression coefficients are equal to zero.

F-test for testing significance of the Regression
Model
The <i>F</i>-statistic is given by
<math display="block">F = \frac{MSR}{MSE} = \frac{SSR/1}{SSE/n - 2}</math>
The test statistic follows F-distribution with (1,n-2)
degrees of freedom

Number of
Number
Delivery
Delivery
Observation
of Cases, x
Observation
Time, y
Time, y
Cases, x
14
16.68
19.75
6
3 3
11.50
15
24.00
2
9
3
12.03
16
29.00
10
15.35
14.88
4
17
6
4
5
13.75
18
19.00
6
3
19
18.11
9.50
6
7
17
8.00
2
20
35.10
7
8
17.83
21
17.90
10
9
22
79.24
30
52.32
26
5
23
10
21.50
18.75
9
8
16
11
40.33
24
19.83
25
4
12
10
10.75
21.00
13
13.50
4

Regression Analysis: Time versus Cases
The regression equation is
<math>Time = 3.32 + 2.18</math> Cases
T
P
Predictor
Coef
SE Coef
1.371 2.42 0.024
Constant
3.321
2.1762
0.1240 17.55 0.000
Cases
<math>S = 4.18140</math> R- <math>Sq = 93.0%</math> R- <math>Sq(adj) = 92.7%</math>
Analysis of Variance
F
SS
P
DF
MS
Source
1
5382.4
5382.4 307.85 0.000
Regression
Residual Error 23
402.1
17.5
24
5784.5
Total

Ex: For the following data:
a) Analyze the relationship between years of education
Y (salary
X (years of
and salary (fit a regression line)
b) Test whether the regression model is significant
'000 Rs.)
education)
(hint: carry out ANOVA and F-test) at alpha=0.05.
9
15
c) Explain the model's goodness of fit (hint: compute
10
20
R-square)
11
24
d) Test whether the slope of the regression is
12
30
statistically significant alpha=0.05.
13
35
e) Calculate the interval estimate of the regression
coefficient <math>(\beta_1)</math>.
5
19
4
10